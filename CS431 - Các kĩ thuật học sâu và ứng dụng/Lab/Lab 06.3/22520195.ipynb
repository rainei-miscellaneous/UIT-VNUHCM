{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5d0e906-ac40-4149-a9e4-7a5ec8fabc92",
   "metadata": {},
   "source": [
    "NEURAL MACHINE TRANSLATION WITH TRANSFORMER\n",
    "Dịch các câu từ tiếng Anh sang tiếng Việt sử dụng mô hình Transformer\n",
    "\n",
    "Ở các bài tập trước, chúng ta đã giải quyết bài toán dịch máy bằng cách sử dụng mô hình Sequence-to-Sequence (Seq2Seq) cùng với cơ chế Attention. Nếu như mô hình Seq2Seq cho khả năng dịch thuật còn hạn chế, thì cơ chế Attention đã cải thiện phần nào hiệu năng thông qua việc tập trung sự chú ý vào các vùng chứa thông tin quan trọng trong câu. Tuy nhiên, chúng ta mới chỉ thực nghiệm cơ chế Attention bằng cách gắn thêm một/một số lượng Attention Head vào mô hình để gán trọng số cho các đặc trưng được trích xuất từ các lớp trước đó thông qua mạng RNN. Vậy, liệu chúng ta có thể thay thế tất cả các lớp CNN/RNN trước đó thành các lớp kiến trúc Attention hay không? Và nếu làm như vậy thì có giúp mô hình dịch máy học tốt hơn hay không?\n",
    "\n",
    "Câu trả lời đã được công bố trong công trình [\"Attention is All you Need\"](https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) của Ashish Vaswani và cộng sự, năm 2017, giới thiệu về kiến trúc Transformer.\n",
    "Transformer là một kiến trúc mạng học sâu mà ở đó các lớp CNN hay RNN được thay thế bằng các mô-đun self-attention. Cơ chế self-attention cho phép mô hình Transformer truyền tải thông tin một cách dễ dàng qua các chuỗi đầu vào. Hình 1 dưới đây thể hiện sự khác biệt về kiến trúc mô hình giữa RNN+Attention và Transformer.\n",
    "\n",
    "Lưu ý: Nên thay đổi runtime sang GPU runtime để quá trình huấn luyện có thể nhanh hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41851592-3bc4-4ae8-821d-f508502b6adc",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "  <th> Mô hình <a href=https://www.tensorflow.org/text/tutorials/nmt_with_attention>RNN+Attention</a></th>\n",
    "  <th>Một lớp Transformer</th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img width=411 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN+attention-words.png\"/>\n",
    "  </td>\n",
    "  <td>\n",
    "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "818f11ff-3a9d-439d-b394-777af66ddba2",
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-04-10T13:51:31.911408Z",
     "iopub.status.busy": "2025-04-10T13:51:31.911074Z",
     "iopub.status.idle": "2025-04-10T13:51:31.955719Z",
     "shell.execute_reply": "2025-04-10T13:51:31.954974Z",
     "shell.execute_reply.started": "2025-04-10T13:51:31.911384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# @title Khởi tạo các hàm và lớp cần thiết dựa trên các code trước đây\n",
    "# import from custom utils.py\n",
    "# from utils import data_preprocessing, Translator, evaluate\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
    "import pandas as pd\n",
    "from keras.models import load_model, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def masked_loss(label, pred):\n",
    "    mask = tf.argmax(label, axis=-1) != 0\n",
    "    loss_object = tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=False, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.argmax(label, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != 0\n",
    "\n",
    "    match = match & mask\n",
    "\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n",
    "\n",
    "def fix_random_seed(seed_value = 42):\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "def data_preprocessing(lines):\n",
    "    eng_train_lines = list()\n",
    "    vie_train_lines = list()\n",
    "\n",
    "    eng_val_lines = list()\n",
    "    vie_val_lines = list()\n",
    "\n",
    "    eng_full_lines = list()\n",
    "    vie_full_lines = list()\n",
    "\n",
    "    train_data, val_data = train_test_split(lines, test_size=0.1)\n",
    "\n",
    "\n",
    "    eng_train_lines = list(train_data.eng)\n",
    "    vie_train_lines = [ '<START> ' + line_vie + ' <END>'  for line_vie in train_data.vie]\n",
    "\n",
    "    eng_val_lines = list(val_data.eng)\n",
    "    vie_val_lines = [ '<START> ' + line_vie + ' <END>'  for line_vie in val_data.vie]\n",
    "\n",
    "    eng_full_lines = eng_train_lines + eng_val_lines\n",
    "    vie_full_lines = vie_train_lines + vie_val_lines\n",
    "\n",
    "    eng_tokenizer = preprocessing.text.Tokenizer()\n",
    "    eng_tokenizer.fit_on_texts( eng_full_lines )\n",
    "\n",
    "    tokenized_full_eng_lines = eng_tokenizer.texts_to_sequences( eng_full_lines )\n",
    "    tokenized_train_eng_lines = eng_tokenizer.texts_to_sequences( eng_train_lines )\n",
    "    tokenized_val_eng_lines = eng_tokenizer.texts_to_sequences( eng_val_lines )\n",
    "    max_input_length = np.array( [len( token_seq ) for token_seq in tokenized_full_eng_lines] ).max()\n",
    "\n",
    "    vie_tokenizer = preprocessing.text.Tokenizer()\n",
    "    vie_tokenizer.fit_on_texts( vie_full_lines )\n",
    "    tokenized_full_vie_lines = vie_tokenizer.texts_to_sequences( vie_full_lines )\n",
    "    tokenized_train_vie_lines = vie_tokenizer.texts_to_sequences( vie_train_lines )\n",
    "    tokenized_val_vie_lines = vie_tokenizer.texts_to_sequences( vie_val_lines )\n",
    "    max_output_length = np.array( [len( token_seq ) for token_seq in tokenized_full_vie_lines] ).max()\n",
    "\n",
    "    padded_train_eng_lines = preprocessing.sequence.pad_sequences( tokenized_train_eng_lines, maxlen=max(max_input_length, max_output_length), padding='post' )\n",
    "    encoder_train_input_data = np.array( padded_train_eng_lines )\n",
    "\n",
    "    padded_val_eng_lines = preprocessing.sequence.pad_sequences( tokenized_val_eng_lines, maxlen=max(max_input_length, max_output_length) , padding='post' )\n",
    "    encoder_val_input_data = np.array( padded_val_eng_lines )\n",
    "\n",
    "    eng_word_dict = eng_tokenizer.word_index\n",
    "    num_eng_tokens = len( eng_word_dict )+1\n",
    "\n",
    "    print( 'Độ dài lớn nhất của English là {}'.format( max_input_length ))\n",
    "    print( 'Kích thước dữ liệu của Encoder  -> {}'.format( encoder_train_input_data.shape ))\n",
    "    print( 'Số lượng English tokens = {}'.format( num_eng_tokens))\n",
    "\n",
    "\n",
    "\n",
    "    # sử dụng pad_sequences để cố định kích thước output của decoder\n",
    "    padded_train_vie_lines = preprocessing.sequence.pad_sequences( tokenized_train_vie_lines, maxlen=max(max_input_length, max_output_length) , padding='post' )\n",
    "    decoder_train_input_data = np.array( padded_train_vie_lines )\n",
    "\n",
    "    padded_val_vie_lines = preprocessing.sequence.pad_sequences( tokenized_val_vie_lines, maxlen=max(max_input_length, max_output_length) , padding='post' )\n",
    "    decoder_val_input_data = np.array( padded_val_vie_lines )\n",
    "\n",
    "    vie_word_dict = vie_tokenizer.word_index\n",
    "    num_vie_tokens = len( vie_word_dict )+1\n",
    "\n",
    "\n",
    "    print( 'Độ dài lớn nhất của tiếng việt là {}'.format( max_output_length ))\n",
    "    print( 'kích thước dữ liệu đầu vào của Decoder -> {}'.format( decoder_train_input_data.shape ))\n",
    "    print( 'Số lượng Vietnamese tokens = {}'.format( num_vie_tokens))\n",
    "\n",
    "\n",
    "\n",
    "    input_decoder_target_data = list()\n",
    "\n",
    "    # TODO: chúng ta sẽ loại bỏ '<START> ' đầu tiên của các dòng trong biến `tokenized_vie_lines`\n",
    "    # và thêm vào `input_decoder_target_data`\n",
    "\n",
    "    \n",
    "    for token_seq in tokenized_train_vie_lines:\n",
    "        input_decoder_target_data.append( token_seq[ 1 : ] )\n",
    "        # input_decoder_target_data.append( token_seq[ 1 : ] )\n",
    "    \n",
    "\n",
    "    padded_vie_lines = preprocessing.sequence.pad_sequences(input_decoder_target_data , maxlen=max_output_length, padding='post' )\n",
    "    onehot_vie_lines = utils.to_categorical( padded_vie_lines , num_vie_tokens )\n",
    "    decoder_target_data = np.array( onehot_vie_lines )\n",
    "    print( 'Decoder target data shape -> {}'.format( decoder_target_data.shape))\n",
    "\n",
    "    map_vie_i2w = {val: key for key, val in vie_word_dict.items()}\n",
    "    val_target = [[[map_vie_i2w[i] for i in line[1:-1]]] for line in tokenized_val_vie_lines]\n",
    "    return (encoder_train_input_data, decoder_train_input_data, decoder_target_data), (encoder_val_input_data,  decoder_val_input_data, val_target), \\\n",
    "                    (eng_word_dict, vie_word_dict), (num_eng_tokens, num_vie_tokens), (eng_tokenizer, vie_tokenizer), (max_input_length, max_output_length)\n",
    "\n",
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "    # TODO: `angle` sẽ được tính thông qua công thức trên\n",
    "    # với các input là `depth` và `length`\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "    angle = 1 / (10000**depths)         # (1, depth)\n",
    "    angle = positions * angle      # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle), np.cos(angle)],\n",
    "        axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha  = None # MultiHeadAttention\n",
    "        self.layernorm = None\n",
    "        self.add = None\n",
    "\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            use_causal_mask = True)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq = None # nên `tf.keras.Sequential` để chứa 2 lớp `Dense`\n",
    "        self.add = None\n",
    "        self.layer_norm = None\n",
    "        self.seq = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(dff, activation='relu'),\n",
    "          tf.keras.layers.Dense(d_model),\n",
    "          tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads,\n",
    "                dff, vocab_size, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "\n",
    "      self.d_model = d_model\n",
    "      self.num_layers = num_layers\n",
    "\n",
    "      self.pos_embedding = PositionalEmbedding(\n",
    "          vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "      self.enc_layers = [\n",
    "          EncoderLayer(d_model=d_model,\n",
    "                      num_heads=num_heads,\n",
    "                      dff=dff,\n",
    "                      dropout_rate=dropout_rate)\n",
    "          for _ in range(num_layers)]\n",
    "      self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [None, None, self.d_model]\n",
    "\n",
    "    def call(self, x):\n",
    "        # `x` is token-IDs shape: (batch, seq_len)\n",
    "        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "        # Add dropout.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                *,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dff,\n",
    "                dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.causal_self_attention = CausalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context):\n",
    "        x = self.causal_self_attention(x=x)\n",
    "        x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "        # Cache the last attention scores for plotting later\n",
    "        self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "                dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                                d_model=d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                        dff=dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "        # `x` is token-IDs shape (batch, target_seq_len)\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x  = self.dec_layers[i](x, context)\n",
    "\n",
    "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "        return x\n",
    "\n",
    "class Translator:\n",
    "\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.0, vie_word_dict=None, tokenizers=None, loss = masked_loss):\n",
    "        fix_random_seed()\n",
    "        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=input_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=target_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size, activation=tf.keras.activations.softmax)\n",
    "\n",
    "        self.eng_tokenizer = tokenizers[0]\n",
    "        self.vie_tokenizer = tokenizers[1]\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.map_vie_i2w = {val: key for key, val in vie_word_dict.items()}\n",
    "        self.vie_word_dict = vie_word_dict\n",
    "        self.loss = loss\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        encoder_inputs = tf.keras.layers.Input(shape=( None,))\n",
    "        decoder_inputs = tf.keras.layers.Input(shape=( None,))\n",
    "\n",
    "        context = self.encoder(encoder_inputs)  # (batch_size, context_len, d_model)\n",
    "\n",
    "        output = self.decoder(decoder_inputs, context)  # (batch_size, target_len, d_model)\n",
    "\n",
    "        # Final linear layer output.\n",
    "        output = self.final_layer(output)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "        try:\n",
    "            # Delete the keras mask, so keras doesn't scale the loss+accuracy.\n",
    "            del output._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        self.model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "\n",
    "\n",
    "    # Load mô hình từ file\n",
    "    def load(self, model_file):\n",
    "        self.model = load_model(model_file)\n",
    "\n",
    "    # Lưu mô hình hiện tại xuống file\n",
    "    def save(self, model_file):\n",
    "        self.model.save(model_file)\n",
    "\n",
    "    # Tóm tắt kiến trúc mạng\n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    # Thử nghiệm mô hình với dữ liệu ảnh đầu vào\n",
    "    def predict(self, x_test):\n",
    "        return self.model.predict(x_test)\n",
    "\n",
    "    def train(self, encoder_input_data , decoder_input_data, decoder_target_data):\n",
    "        # Các hyper-parameter ở đây được chỉnh để có thể so sánh với mô hình trước.\n",
    "        # Nếu muốn, các bạn có thể tinh chỉnh để đạt hiệu suất cao hơn.\n",
    "        learning_rate = CustomSchedule(self.d_model)\n",
    "        # print(learning_rate)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                        epsilon=1e-9)\n",
    "        self.model.compile(\n",
    "                loss=self.loss,\n",
    "                optimizer=optimizer,\n",
    "                metrics=[masked_accuracy],\n",
    "                )\n",
    "        self.model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=8, epochs=12)\n",
    "\n",
    "    def translate(self, input, return_attention=False):\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        input = tf.convert_to_tensor(input, dtype=tf.int64)\n",
    "\n",
    "        start, end = self.vie_word_dict['start'], self.vie_word_dict['end']\n",
    "        output_array = output_array.write(0, [start])\n",
    "\n",
    "        for i in tf.range(max_output_length):\n",
    "            output = tf.transpose(output_array.stack())\n",
    "            predictions = self.model([input, output], training=False)\n",
    "\n",
    "            # Select the last token from the `seq_len` dimension.\n",
    "\n",
    "            predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "            predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "            # Concatenate the `predicted_id` to the output which is given to the\n",
    "            if predicted_id[0] == end or predicted_id[0]==0:\n",
    "                break\n",
    "\n",
    "            output_array = output_array.write(i+1, predicted_id[0])\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        # print(output)\n",
    "        text = [self.map_vie_i2w.get(i, '') for i in output[0].numpy()[1:]]  # Shape: `()`.\n",
    "        if return_attention:\n",
    "            self.model([input, output], training=False)\n",
    "            attention_weights = self.decoder.last_attn_scores\n",
    "            attention_heads = tf.squeeze(attention_weights, 0)\n",
    "            return text, attention_heads\n",
    "        return text\n",
    "\n",
    "    def plot_attention(self, text, **kwargs):\n",
    "        assert isinstance(text, str)\n",
    "\n",
    "        input = self.eng_tokenizer.texts_to_sequences([text])\n",
    "        input = preprocessing.sequence.pad_sequences(input, maxlen=max_input_length , padding='post' )\n",
    "        output, attention = self.translate(input, return_attention=True)\n",
    "        attention_weights = tf.concat(attention, 0)\n",
    "        context = text.split()\n",
    "\n",
    "        for i in range(len(attention_weights)):\n",
    "            attention = attention_weights[i][:, :len(context)]\n",
    "            fig = plt.figure(figsize=(10, 10))\n",
    "            ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "            ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
    "\n",
    "            fontdict = {'fontsize': 14}\n",
    "\n",
    "            ax.set_xticklabels([''] + context, fontdict=fontdict, rotation=90)\n",
    "            ax.set_yticklabels([''] + output, fontdict=fontdict)\n",
    "\n",
    "            ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "            ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "            ax.set_ylabel('Output text')\n",
    "            ax.set_xlabel(f'Head {i}')\n",
    "\n",
    "\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "def evaluate(model, encoder_val_input_data, val_target, num_sample=None):\n",
    "    num_sample = encoder_val_input_data.shape[0] if num_sample==None else num_sample\n",
    "\n",
    "    predict_translation = []\n",
    "    eng_sentence = []\n",
    "\n",
    "    for i in tqdm(range( num_sample ) ):\n",
    "        decoded_translation = model.translate(encoder_val_input_data[ i ][None,...])\n",
    "        eng_sentence.append(encoder_val_input_data[ i ])\n",
    "        predict_translation.append( decoded_translation )\n",
    "\n",
    "\n",
    "    predict = [f for f in predict_translation]\n",
    "\n",
    "    references = val_target[:num_sample]\n",
    "    candidates = predict\n",
    "    score = corpus_bleu(references, candidates, weights=(0.5, 0.5))\n",
    "    print('\\nGiá trị bleu score là', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2eb36834-4386-4b04-960b-d7b83a717841",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:51:31.957113Z",
     "iopub.status.busy": "2025-04-10T13:51:31.956868Z",
     "iopub.status.idle": "2025-04-10T13:51:31.982154Z",
     "shell.execute_reply": "2025-04-10T13:51:31.981504Z",
     "shell.execute_reply.started": "2025-04-10T13:51:31.957094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import các thư viện cần thiết\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.models import load_model, Model\n",
    "from keras import layers , activations , models , preprocessing , utils\n",
    "from tensorflow.keras import layers, activations, models, preprocessing, utils\n",
    "fix_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "559cf76d-f2a5-4b03-a5dd-0da0db886f15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-10T13:51:31.985276Z",
     "iopub.status.busy": "2025-04-10T13:51:31.985028Z",
     "iopub.status.idle": "2025-04-10T13:51:32.044636Z",
     "shell.execute_reply": "2025-04-10T13:51:32.043765Z",
     "shell.execute_reply.started": "2025-04-10T13:51:31.985257Z"
    },
    "outputId": "d9079f19-3c31-470f-9219-e69c72c5635a",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>vie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Who's your favorite movie star?</td>\n",
       "      <td>Ngôi sao điện ảnh mà bạn yêu thích là ai thế?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Who's your favorite movie star?</td>\n",
       "      <td>Ngôi sao điện ảnh mà bạn yêu thích là ai vậy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Why aren't you in your uniform?</td>\n",
       "      <td>Tại sao bạn không mặc đồng phục?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Will he be coming this evening?</td>\n",
       "      <td>Tối nay anh ấy có đến không?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Will he be coming this evening?</td>\n",
       "      <td>Tối nay ông ấy có đến không?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  eng  \\\n",
       "4995  Who's your favorite movie star?   \n",
       "4996  Who's your favorite movie star?   \n",
       "4997  Why aren't you in your uniform?   \n",
       "4998  Will he be coming this evening?   \n",
       "4999  Will he be coming this evening?   \n",
       "\n",
       "                                                vie  \n",
       "4995  Ngôi sao điện ảnh mà bạn yêu thích là ai thế?  \n",
       "4996  Ngôi sao điện ảnh mà bạn yêu thích là ai vậy?  \n",
       "4997               Tại sao bạn không mặc đồng phục?  \n",
       "4998                   Tối nay anh ấy có đến không?  \n",
       "4999                   Tối nay ông ấy có đến không?  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = None\n",
    "lines = pd.read_table('./data/vie.txt' , names=['eng' , 'vie' , 'c' ] )\n",
    "lines = lines.drop(['c'] , axis=1 )[0:5000]\n",
    "lines.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ebae55c-1103-4db6-9b7e-2d0d5fe138c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-10T13:51:32.046050Z",
     "iopub.status.busy": "2025-04-10T13:51:32.045820Z",
     "iopub.status.idle": "2025-04-10T13:51:33.259897Z",
     "shell.execute_reply": "2025-04-10T13:51:33.258950Z",
     "shell.execute_reply.started": "2025-04-10T13:51:32.046031Z"
    },
    "outputId": "e9752297-d1b3-42ac-8a9f-100d738eaa1c",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Độ dài lớn nhất của English là 9\n",
      "Kích thước dữ liệu của Encoder  -> (4500, 17)\n",
      "Số lượng English tokens = 2451\n",
      "Độ dài lớn nhất của tiếng việt là 17\n",
      "kích thước dữ liệu đầu vào của Decoder -> (4500, 17)\n",
      "Số lượng Vietnamese tokens = 1861\n",
      "Decoder target data shape -> (4500, 17, 1861)\n"
     ]
    }
   ],
   "source": [
    "# from utils import data_preprocessing\n",
    "train_data, val_data, word_dict, num_tokens, tokenizers, max_length = data_preprocessing(lines)\n",
    "\n",
    "(encoder_train_input_data, decoder_train_input_data, decoder_target_data) = train_data\n",
    "(encoder_val_input_data,  decoder_val_input_data, val_target) = val_data\n",
    "(eng_word_dict, vie_word_dict), (num_eng_tokens, num_vie_tokens) = word_dict, num_tokens\n",
    "(eng_tokenizer, vie_tokenizer), (max_input_length, max_output_length) = tokenizers, max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b17d3970-0507-4c94-a9f2-db8dbb92e55e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:51:33.260951Z",
     "iopub.status.busy": "2025-04-10T13:51:33.260710Z",
     "iopub.status.idle": "2025-04-10T13:51:33.264692Z",
     "shell.execute_reply": "2025-04-10T13:51:33.263708Z",
     "shell.execute_reply.started": "2025-04-10T13:51:33.260930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "d_model = 64\n",
    "dff = 128\n",
    "num_heads = 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71721448-a585-4299-90fa-385902b7476d",
   "metadata": {},
   "source": [
    "## Khởi tạo và huấn luyện mô hình với cài đặt mặc định"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16576514-9477-4663-96b4-10c2237df056",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-10T13:51:33.265783Z",
     "iopub.status.busy": "2025-04-10T13:51:33.265514Z",
     "iopub.status.idle": "2025-04-10T13:58:34.993459Z",
     "shell.execute_reply": "2025-04-10T13:58:34.992571Z",
     "shell.execute_reply.started": "2025-04-10T13:51:33.265762Z"
    },
    "outputId": "b6cc8229-d4c8-49ce-80eb-d88cdc602546",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'global_self_attention_32' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_84' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'feed_forward_64' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'encoder_layer_22' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'global_self_attention_33' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_85' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'feed_forward_65' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'encoder_layer_23' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'causal_self_attention_32' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'cross_attention_32' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_86' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'feed_forward_66' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'decoder_layer_22' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'causal_self_attention_33' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'cross_attention_33' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_87' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'feed_forward_67' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'decoder_layer_23' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 24ms/step - loss: 7.0444 - masked_accuracy: 0.0774\n",
      "Epoch 2/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 5.0813 - masked_accuracy: 0.2098\n",
      "Epoch 3/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 4.2994 - masked_accuracy: 0.3042\n",
      "Epoch 4/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 3.7259 - masked_accuracy: 0.3768\n",
      "Epoch 5/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.2775 - masked_accuracy: 0.4287\n",
      "Epoch 6/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.9109 - masked_accuracy: 0.4749\n",
      "Epoch 7/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.6388 - masked_accuracy: 0.5034\n",
      "Epoch 8/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.4103 - masked_accuracy: 0.5317\n",
      "Epoch 9/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.1245 - masked_accuracy: 0.5740\n",
      "Epoch 10/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1.8979 - masked_accuracy: 0.6081\n",
      "Epoch 11/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1.7080 - masked_accuracy: 0.6365\n",
      "Epoch 12/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1.5692 - masked_accuracy: 0.6545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 2, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n",
      "100%|██████████| 500/500 [05:42<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Giá trị bleu score là 0.37976776481967656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "alpha=Translator(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=num_eng_tokens,\n",
    "    target_vocab_size=num_vie_tokens,\n",
    "    dropout_rate=dropout_rate,\n",
    "    vie_word_dict=vie_word_dict, tokenizers=tokenizers)\n",
    "\n",
    "\n",
    "alpha.build()\n",
    "\n",
    "alpha.train(encoder_train_input_data , decoder_train_input_data, decoder_target_data)\n",
    "evaluate(alpha, encoder_val_input_data, val_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b0e3d6-1e37-45f4-be6e-5008d1f90764",
   "metadata": {},
   "source": [
    "## Skip Connections trong Transformer\n",
    "Trong các mô hình Transformer, chúng ta đều biết rằng các kết nối bỏ qua (skip connections) đóng vai trò quan trọng trong việc duy trì và truyền tải thông tin qua các tầng mạng. Tuy nhiên, câu hỏi đặt ra là điều gì sẽ xảy ra nếu chúng ta thay đổi hoặc loại bỏ các skip connections này?\n",
    "\n",
    "Trong phần thực nghiệm này, chúng ta sẽ tiến hành hai thí nghiệm khác nhau:\n",
    "  - Loại bỏ các Skip Connections\n",
    "  - Thay thế add trong skip connection bằng concatinate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e60e93-6e30-486b-90c8-d6930252987c",
   "metadata": {},
   "source": [
    "### Loại bỏ các Skip Connections:\n",
    "Bằng cách loại bỏ hoàn toàn các skip connections, chúng ta sẽ kiểm tra xem sự thiếu vắng của chúng ảnh hưởng như thế nào đến khả năng huấn luyện và hiệu suất của mô hình. Điều này giúp chúng ta hiểu rõ hơn về tầm quan trọng của skip connections trong việc duy trì thông tin và giảm thiểu hiện tượng biến mất gradient trong quá trình huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2381399a-8d7d-436d-af2b-8bc34ec85bbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:58:34.994780Z",
     "iopub.status.busy": "2025-04-10T13:58:34.994451Z",
     "iopub.status.idle": "2025-04-10T13:58:35.002766Z",
     "shell.execute_reply": "2025-04-10T13:58:35.001965Z",
     "shell.execute_reply.started": "2025-04-10T13:58:34.994742Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: Định nghĩa các lớp có sử dụng skip-connection trong kiến trúc cũ:\n",
    "# - EtBaseAttention thay thế cho BaseAttention\n",
    "# - EtCrossAttention thay thế cho CrossAttention\n",
    "# - EtGlobalSelfAttention thay thế cho GlobalSelfAttention\n",
    "# - EtCausalSelfAttention thay thế cho CausalSelfAttention,\n",
    "# - EtFeedForward thay thế cho FeedForward\n",
    "\n",
    "class EtBaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        ### BEGIN SOLUTION\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        ### END SOLUTION\n",
    "\n",
    "class EtCrossAttention(EtBaseAttention):\n",
    "    def call(self, x, context):\n",
    "        ### BEGIN SOLUTION\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "        attn_output = self.layernorm(attn_output)\n",
    "        return attn_output\n",
    "        ### END SOLUTION\n",
    "\n",
    "class EtGlobalSelfAttention(EtBaseAttention):\n",
    "    ### BEGIN SOLUTION\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = attn_output\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "    ### END SOLUTION\n",
    "\n",
    "class EtCausalSelfAttention(EtBaseAttention):\n",
    "    def call(self, x):\n",
    "        ### BEGIN SOLUTION\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            use_causal_mask = True)\n",
    "        x = attn_output\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "        ### END SOLUTION\n",
    "\n",
    "class EtFeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.dff = dff\n",
    "        self.d_model = d_model\n",
    "        ### BEGIN SOLUTION\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def call(self, x):\n",
    "        ### BEGIN SOLUTION\n",
    "        return self.layer_norm(self.seq(x))\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92d7e4e8-8acd-4fdf-8c65-eb436db35cb5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-10T13:58:35.003890Z",
     "iopub.status.busy": "2025-04-10T13:58:35.003584Z",
     "iopub.status.idle": "2025-04-10T14:05:35.016710Z",
     "shell.execute_reply": "2025-04-10T14:05:35.015772Z",
     "shell.execute_reply.started": "2025-04-10T13:58:35.003866Z"
    },
    "outputId": "9a24eba5-dde6-4986-ec17-fc5d96f99105",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'et_global_self_attention_2' (of type EtGlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_95' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'et_feed_forward_4' (of type EtFeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'et_encoder_layer_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'et_encoder_layer_2' (of type EtEncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'et_global_self_attention_3' (of type EtGlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_97' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'et_feed_forward_5' (of type EtFeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'et_encoder_layer_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'et_encoder_layer_3' (of type EtEncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'et_encoder_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'et_causal_self_attention_2' (of type EtCausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'et_cross_attention_2' (of type EtCrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_101' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'et_feed_forward_6' (of type EtFeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'et_decoder_layer_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'et_decoder_layer_2' (of type EtDecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'et_causal_self_attention_3' (of type EtCausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'et_cross_attention_3' (of type EtCrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_103' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'et_feed_forward_7' (of type EtFeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'et_decoder_layer_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'et_decoder_layer_3' (of type EtDecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'et_decoder_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 24ms/step - loss: 7.0379 - masked_accuracy: 0.0888\n",
      "Epoch 2/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.4550 - masked_accuracy: 0.1336\n",
      "Epoch 3/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.3316 - masked_accuracy: 0.1322\n",
      "Epoch 4/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 5.2470 - masked_accuracy: 0.1292\n",
      "Epoch 5/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 5.1916 - masked_accuracy: 0.1290\n",
      "Epoch 6/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 5.1535 - masked_accuracy: 0.1312\n",
      "Epoch 7/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 5.1155 - masked_accuracy: 0.1314\n",
      "Epoch 8/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 5.0784 - masked_accuracy: 0.1276\n",
      "Epoch 9/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 5.0165 - masked_accuracy: 0.1295\n",
      "Epoch 10/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 4.9577 - masked_accuracy: 0.1297\n",
      "Epoch 11/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 4.9134 - masked_accuracy: 0.1280\n",
      "Epoch 12/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 4.8806 - masked_accuracy: 0.1286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [05:40<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Giá trị bleu score là 0.21608231958379406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "class EtEncoderLayer(EncoderLayer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.self_attention = EtGlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = EtFeedForward(d_model, dff)\n",
    "\n",
    "\n",
    "class EtEncoder(Encoder):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.enc_layers = [\n",
    "            EtEncoderLayer(d_model=d_model,\n",
    "                        num_heads=num_heads,\n",
    "                        dff=dff,\n",
    "                        dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "class EtDecoderLayer(DecoderLayer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.causal_self_attention = EtCausalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.cross_attention = EtCrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = EtFeedForward(d_model, dff)\n",
    "\n",
    "\n",
    "\n",
    "class EtDecoder(Decoder):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dec_layers = [\n",
    "            EtDecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                        dff=dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "\n",
    "class Eta(Translator):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.0, vie_word_dict=None, tokenizers=None):\n",
    "        super().__init__(num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate, vie_word_dict, tokenizers)\n",
    "        self.encoder = EtEncoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=input_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "        self.decoder = EtDecoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=target_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "eta=Eta(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=num_eng_tokens,\n",
    "    target_vocab_size=num_vie_tokens,\n",
    "    dropout_rate=dropout_rate,\n",
    "    vie_word_dict=vie_word_dict,\n",
    "    tokenizers=tokenizers)\n",
    "\n",
    "eta.build()\n",
    "\n",
    "eta.train(encoder_train_input_data , decoder_train_input_data, decoder_target_data)\n",
    "evaluate(eta, encoder_val_input_data, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1533c87-c57f-47c4-bf84-243cc30211bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T14:05:35.019210Z",
     "iopub.status.busy": "2025-04-10T14:05:35.018942Z",
     "iopub.status.idle": "2025-04-10T14:05:35.023434Z",
     "shell.execute_reply": "2025-04-10T14:05:35.022608Z",
     "shell.execute_reply.started": "2025-04-10T14:05:35.019164Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### BEGIN PUBLIC TESTS\n",
    "for e in eta.encoder.enc_layers:\n",
    "    assert not any(isinstance(value, tf.keras.layers.Add) for value in e.self_attention.__dict__.values())\n",
    "    assert not any(isinstance(value, tf.keras.layers.Add) for value in e.ffn.__dict__.values())\n",
    "### END PUBLIC TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dafa5081-8949-44c5-aa6c-82d77ae7e90e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T14:05:35.024890Z",
     "iopub.status.busy": "2025-04-10T14:05:35.024594Z",
     "iopub.status.idle": "2025-04-10T14:05:35.038850Z",
     "shell.execute_reply": "2025-04-10T14:05:35.037975Z",
     "shell.execute_reply.started": "2025-04-10T14:05:35.024860Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theo mình thì:  hiệu suất eta thấp hơn alpha k% (10<=k<20)\n"
     ]
    }
   ],
   "source": [
    "options = {0: 'hiệu suất eta xấp xỉ alpha (không chêch lệch quá 1%)',\n",
    "           1: 'hiệu suất eta thấp hơn alpha k% (1<k<5)',\n",
    "           2: 'hiệu suất eta thấp hơn alpha k% (5<=k<10)',\n",
    "           3: 'hiệu suất eta thấp hơn alpha k% (10<=k<20)',\n",
    "           4: 'hiệu suất eta thấp hơn alpha trên 20%',\n",
    "           5: 'eta gần như không học gì (hiệu suất thấp hơn 10%)'}\n",
    "your_choice = None\n",
    "# BEGIN SOLUTION\n",
    "your_choice = 5\n",
    "# END SOLUTION\n",
    "print(\"Theo mình thì: \", options[your_choice])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450d13ee-0e0b-4887-a4be-ececc0431b8c",
   "metadata": {},
   "source": [
    "### Thay Add trong skip-connection bằng Concatenate\n",
    "Thông thường, các skip connections được thực hiện bằng cách cộng (add) đầu vào của lớp với đầu ra của lớp đó. Điều này giúp duy trì thông tin và giảm thiểu hiện tượng biến mất gradient trong quá trình huấn luyện. Tuy nhiên, câu hỏi đặt ra là điều gì sẽ xảy ra nếu chúng ta thay thế phép cộng (add) trong các skip connections bằng phép nối (concatenate)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceee205-682f-4cc7-b070-2dc4652fc82c",
   "metadata": {},
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b53db85c-7af4-4664-8ee7-62f17f708a04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T14:19:20.609604Z",
     "iopub.status.busy": "2025-04-10T14:19:20.609250Z",
     "iopub.status.idle": "2025-04-10T14:19:20.619630Z",
     "shell.execute_reply": "2025-04-10T14:19:20.618951Z",
     "shell.execute_reply.started": "2025-04-10T14:19:20.609579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: Định nghĩa các lớp có sử dụng skip-connection trong kiến trúc cũ bằng cách thay Add bằng Concatenate (sử dụng mudule `concatenate` trong `tf.keras.layers`):\n",
    "# - EtBaseAttention thay thế cho BaseAttention\n",
    "# - EtCrossAttention thay thế cho CrossAttention\n",
    "# - EtGlobalSelfAttention thay thế cho GlobalSelfAttention\n",
    "# - EtCausalSelfAttention thay thế cho CausalSelfAttention,\n",
    "# - EtFeedForward thay thế cho FeedForward\n",
    "\n",
    "class ZBaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        ### BEGIN SOLUTION\n",
    "        self.mha  = None # MultiHeadAttention\n",
    "        self.layernorm = None\n",
    "        self.concatenate = None\n",
    "        \n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.concatenate = tf.keras.layers.Concatenate()\n",
    "        ### END SOLUTION\n",
    "\n",
    "class ZCrossAttention(ZBaseAttention):\n",
    "    def call(self, x, context):\n",
    "        ### BEGIN SOLUTION\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "\n",
    "        x = self.concatenate([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "        ### END SOLUTION\n",
    "class ZGlobalSelfAttention(ZBaseAttention):\n",
    "    def call(self, x):\n",
    "        ### BEGIN SOLUTION\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.concatenate([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "        ### END SOLUTION\n",
    "class ZCausalSelfAttention(ZBaseAttention):\n",
    "    def call(self, x):\n",
    "        ### BEGIN SOLUTION\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            use_causal_mask = True)\n",
    "        x = self.concatenate([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "        ### END SOLUTION\n",
    "\n",
    "\n",
    "class ZFeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.dff = dff\n",
    "        self.d_model = d_model\n",
    "\n",
    "        ### BEGIN SOLUTION\n",
    "        self.seq = None # nên `tf.keras.Sequential` để chứa 2 lớp `Dense`\n",
    "        self.concatenate = None\n",
    "        self.layer_norm = None\n",
    "        self.seq = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(dff, activation='relu'),\n",
    "          tf.keras.layers.Dense(d_model),\n",
    "          tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.concatenate = tf.keras.layers.Concatenate()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def call(self, x):\n",
    "        ### BEGIN SOLUTION\n",
    "        x = self.concatenate([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff79d3ea-430f-4f10-be74-01bc71e01847",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-10T14:19:20.620791Z",
     "iopub.status.busy": "2025-04-10T14:19:20.620590Z",
     "iopub.status.idle": "2025-04-10T14:19:20.849698Z",
     "shell.execute_reply": "2025-04-10T14:19:20.848404Z",
     "shell.execute_reply.started": "2025-04-10T14:19:20.620773Z"
    },
    "outputId": "f3193fe0-21fd-4bb7-8808-fc465a4128fb",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:1383: UserWarning: Layer 'z_encoder_6' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Index out of range using input dim 2; input has only 2 dims for '{{node strided_slice_2}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=7, ellipsis_mask=0, end_mask=5, new_axis_mask=0, shrink_axis_mask=0](10474727, strided_slice_2/stack, strided_slice_2/stack_1, strided_slice_2/stack_2)' with input shapes: [1000,64], [3], [3], [3] and with computed input tensors: input[3] = <1 1 1>.''\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'z_encoder_6', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling ZEncoder.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'z_encoder_6' (of type ZEncoder). Either the `ZEncoder.call()` method is incorrect, or you need to implement the `ZEncoder.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nIndex out of range using input dim 2; input has only 2 dims for '{{node strided_slice_2}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=7, ellipsis_mask=0, end_mask=5, new_axis_mask=0, shrink_axis_mask=0](10474786, strided_slice_2/stack, strided_slice_2/stack_1, strided_slice_2/stack_2)' with input shapes: [1000,64], [3], [3], [3] and with computed input tensors: input[3] = <1 1 1>.\u001b[0m\n\nArguments received by ZEncoder.call():\n  • args=('<KerasTensor shape=(None, None), dtype=float32, sparse=False, name=keras_tensor_266>',)\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-b6f8e5904302>\u001b[0m in \u001b[0;36m<cell line: 104>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0mzeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0mzeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_train_input_data\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdecoder_train_input_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_target_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-f597182e52c4>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mdecoder_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, context_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, target_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-b6f8e5904302>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling ZEncoder.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'z_encoder_6' (of type ZEncoder). Either the `ZEncoder.call()` method is incorrect, or you need to implement the `ZEncoder.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nIndex out of range using input dim 2; input has only 2 dims for '{{node strided_slice_2}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=7, ellipsis_mask=0, end_mask=5, new_axis_mask=0, shrink_axis_mask=0](10474786, strided_slice_2/stack, strided_slice_2/stack_1, strided_slice_2/stack_2)' with input shapes: [1000,64], [3], [3], [3] and with computed input tensors: input[3] = <1 1 1>.\u001b[0m\n\nArguments received by ZEncoder.call():\n  • args=('<KerasTensor shape=(None, None), dtype=float32, sparse=False, name=keras_tensor_266>',)\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "class ZEncoderLayer(EncoderLayer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.self_attention = ZGlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = ZFeedForward(d_model, dff)\n",
    "\n",
    "\n",
    "class ZEncoder(Encoder):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.enc_layers = [\n",
    "            ZEncoderLayer(d_model=d_model,\n",
    "                        num_heads=num_heads,\n",
    "                        dff=dff,\n",
    "                        dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "class ZDecoderLayer(DecoderLayer):\n",
    "    def __init__(self,\n",
    "                **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.causal_self_attention = ZCausalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.cross_attention = ZCrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = ZFeedForward(d_model, dff)\n",
    "\n",
    "\n",
    "\n",
    "class ZDecoder(Decoder):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dec_layers = [\n",
    "            ZDecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                        dff=dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "class Zeta(Translator):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.0, vie_word_dict=None, tokenizers=None):\n",
    "        super().__init__(num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate, vie_word_dict, tokenizers)\n",
    "        self.encoder = ZEncoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=input_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "        self.decoder = ZDecoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=target_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "zeta=Zeta(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=num_eng_tokens,\n",
    "    target_vocab_size=num_vie_tokens,\n",
    "    dropout_rate=dropout_rate,\n",
    "    vie_word_dict=vie_word_dict,\n",
    "    tokenizers=tokenizers)\n",
    "\n",
    "\n",
    "zeta.build()\n",
    "\n",
    "zeta.train(encoder_train_input_data , decoder_train_input_data, decoder_target_data)\n",
    "evaluate(zeta, encoder_val_input_data, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f33690-4067-4ba8-87e9-61d87fc0f835",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-10T14:19:20.850083Z",
     "iopub.status.idle": "2025-04-10T14:19:20.850355Z",
     "shell.execute_reply": "2025-04-10T14:19:20.850247Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### BEGIN PUBLIC TESTS\n",
    "x = tf.random.uniform((1, 10, 64))\n",
    "\n",
    "custom_ffn = ZFeedForward(64,128)\n",
    "ffn = FeedForward(64,128)\n",
    "\n",
    "out_ffn = ffn(x)\n",
    "out_custom_ffn = custom_ffn(x)\n",
    "\n",
    "\n",
    "assert out_ffn.shape[2]*2 == out_custom_ffn.shape[2]\n",
    "### END PUBLIC TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c62bb-eea2-4469-8a9b-fbcf13dcae7f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-10T14:19:20.851113Z",
     "iopub.status.idle": "2025-04-10T14:19:20.851458Z",
     "shell.execute_reply": "2025-04-10T14:19:20.851304Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "options = {0: 'hiệu suất zeta xấp xỉ alpha (không chêch lệch quá 1%)',\n",
    "           1: 'hiệu suất zeta thấp hơn alpha k% (1<k<5)',\n",
    "           2: 'hiệu suất zeta thấp hơn alpha k% (5<=k<10)',\n",
    "           3: 'hiệu suất zeta thấp hơn alpha k% (10<=k<20)',\n",
    "           4: 'hiệu suất zeta thấp hơn alpha trên 20%',\n",
    "           5: 'zeta gần như không học gì (hiệu suất thấp hơn 10%)'}\n",
    "your_choice = None\n",
    "# BEGIN SOLUTION\n",
    "your_choice = 1\n",
    "# END SOLUTION\n",
    "print(\"Theo mình thì: \", options[your_choice])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7086270,
     "sourceId": 11328329,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
