{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "475d7d2f-e79b-45f2-997d-9521fcfb6b7b",
   "metadata": {},
   "source": [
    "NEURAL MACHINE TRANSLATION WITH TRANSFORMER\n",
    "Dịch các câu từ tiếng Anh sang tiếng Việt sử dụng mô hình Transformer\n",
    "\n",
    "Ở các bài tập trước, chúng ta đã giải quyết bài toán dịch máy bằng cách sử dụng mô hình Sequence-to-Sequence (Seq2Seq) cùng với cơ chế Attention. Nếu như mô hình Seq2Seq cho khả năng dịch thuật còn hạn chế, thì cơ chế Attention đã cải thiện phần nào hiệu năng thông qua việc tập trung sự chú ý vào các vùng chứa thông tin quan trọng trong câu. Tuy nhiên, chúng ta mới chỉ thực nghiệm cơ chế Attention bằng cách gắn thêm một/một số lượng Attention Head vào mô hình để gán trọng số cho các đặc trưng được trích xuất từ các lớp trước đó thông qua mạng RNN. Vậy, liệu chúng ta có thể thay thế tất cả các lớp CNN/RNN trước đó thành các lớp kiến trúc Attention hay không? Và nếu làm như vậy thì có giúp mô hình dịch máy học tốt hơn hay không?\n",
    "\n",
    "Câu trả lời đã được công bố trong công trình [\"Attention is All you Need\"](https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) của Ashish Vaswani và cộng sự, năm 2017, giới thiệu về kiến trúc Transformer.\n",
    "Transformer là một kiến trúc mạng học sâu mà ở đó các lớp CNN hay RNN được thay thế bằng các mô-đun self-attention. Cơ chế self-attention cho phép mô hình Transformer truyền tải thông tin một cách dễ dàng qua các chuỗi đầu vào. Hình 1 dưới đây thể hiện sự khác biệt về kiến trúc mô hình giữa RNN+Attention và Transformer.\n",
    "\n",
    "Lưu ý: Nên thay đổi runtime sang GPU runtime để quá trình huấn luyện có thể nhanh hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac77e7c7-1500-4504-8eba-d46c428c9012",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "  <th> Mô hình <a href=https://www.tensorflow.org/text/tutorials/nmt_with_attention>RNN+Attention</a></th>\n",
    "  <th>Một lớp Transformer</th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img width=411 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN+attention-words.png\"/>\n",
    "  </td>\n",
    "  <td>\n",
    "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d95c805-7ff2-4bcf-a34b-d68d5d06d90a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T00:55:41.771261Z",
     "iopub.status.busy": "2025-04-11T00:55:41.770528Z",
     "iopub.status.idle": "2025-04-11T00:55:56.392562Z",
     "shell.execute_reply": "2025-04-11T00:55:56.391983Z",
     "shell.execute_reply.started": "2025-04-11T00:55:41.771229Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 00:55:45.190975: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744332945.392283      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744332945.450692      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# @title Khởi tạo các hàm và lớp cần thiết dựa trên các code trước đây\n",
    "# import from custom utils.py\n",
    "# from utils import data_preprocessing, Translator, evaluate\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
    "import pandas as pd\n",
    "from keras.models import load_model, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def masked_loss(label, pred):\n",
    "    mask = tf.argmax(label, axis=-1) != 0\n",
    "    loss_object = tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=False, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.argmax(label, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != 0\n",
    "\n",
    "    match = match & mask\n",
    "\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n",
    "\n",
    "def fix_random_seed(seed_value = 42):\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "def data_preprocessing(lines):\n",
    "    eng_train_lines = list()\n",
    "    vie_train_lines = list()\n",
    "\n",
    "    eng_val_lines = list()\n",
    "    vie_val_lines = list()\n",
    "\n",
    "    eng_full_lines = list()\n",
    "    vie_full_lines = list()\n",
    "\n",
    "    train_data, val_data = train_test_split(lines, test_size=0.1)\n",
    "\n",
    "\n",
    "    eng_train_lines = list(train_data.eng)\n",
    "    vie_train_lines = [ '<START> ' + line_vie + ' <END>'  for line_vie in train_data.vie]\n",
    "\n",
    "    eng_val_lines = list(val_data.eng)\n",
    "    vie_val_lines = [ '<START> ' + line_vie + ' <END>'  for line_vie in val_data.vie]\n",
    "\n",
    "    eng_full_lines = eng_train_lines + eng_val_lines\n",
    "    vie_full_lines = vie_train_lines + vie_val_lines\n",
    "\n",
    "    eng_tokenizer = preprocessing.text.Tokenizer()\n",
    "    eng_tokenizer.fit_on_texts( eng_full_lines )\n",
    "\n",
    "    tokenized_full_eng_lines = eng_tokenizer.texts_to_sequences( eng_full_lines )\n",
    "    tokenized_train_eng_lines = eng_tokenizer.texts_to_sequences( eng_train_lines )\n",
    "    tokenized_val_eng_lines = eng_tokenizer.texts_to_sequences( eng_val_lines )\n",
    "    max_input_length = np.array( [len( token_seq ) for token_seq in tokenized_full_eng_lines] ).max()\n",
    "\n",
    "    vie_tokenizer = preprocessing.text.Tokenizer()\n",
    "    vie_tokenizer.fit_on_texts( vie_full_lines )\n",
    "    tokenized_full_vie_lines = vie_tokenizer.texts_to_sequences( vie_full_lines )\n",
    "    tokenized_train_vie_lines = vie_tokenizer.texts_to_sequences( vie_train_lines )\n",
    "    tokenized_val_vie_lines = vie_tokenizer.texts_to_sequences( vie_val_lines )\n",
    "    max_output_length = np.array( [len( token_seq ) for token_seq in tokenized_full_vie_lines] ).max()\n",
    "\n",
    "    padded_train_eng_lines = preprocessing.sequence.pad_sequences( tokenized_train_eng_lines, maxlen=max(max_input_length, max_output_length), padding='post' )\n",
    "    encoder_train_input_data = np.array( padded_train_eng_lines )\n",
    "\n",
    "    padded_val_eng_lines = preprocessing.sequence.pad_sequences( tokenized_val_eng_lines, maxlen=max(max_input_length, max_output_length) , padding='post' )\n",
    "    encoder_val_input_data = np.array( padded_val_eng_lines )\n",
    "\n",
    "    eng_word_dict = eng_tokenizer.word_index\n",
    "    num_eng_tokens = len( eng_word_dict )+1\n",
    "\n",
    "    print( 'Độ dài lớn nhất của English là {}'.format( max_input_length ))\n",
    "    print( 'Kích thước dữ liệu của Encoder  -> {}'.format( encoder_train_input_data.shape ))\n",
    "    print( 'Số lượng English tokens = {}'.format( num_eng_tokens))\n",
    "\n",
    "\n",
    "\n",
    "    # sử dụng pad_sequences để cố định kích thước output của decoder\n",
    "    padded_train_vie_lines = preprocessing.sequence.pad_sequences( tokenized_train_vie_lines, maxlen=max(max_input_length, max_output_length) , padding='post' )\n",
    "    decoder_train_input_data = np.array( padded_train_vie_lines )\n",
    "\n",
    "    padded_val_vie_lines = preprocessing.sequence.pad_sequences( tokenized_val_vie_lines, maxlen=max(max_input_length, max_output_length) , padding='post' )\n",
    "    decoder_val_input_data = np.array( padded_val_vie_lines )\n",
    "\n",
    "    vie_word_dict = vie_tokenizer.word_index\n",
    "    num_vie_tokens = len( vie_word_dict )+1\n",
    "\n",
    "\n",
    "    print( 'Độ dài lớn nhất của tiếng việt là {}'.format( max_output_length ))\n",
    "    print( 'kích thước dữ liệu đầu vào của Decoder -> {}'.format( decoder_train_input_data.shape ))\n",
    "    print( 'Số lượng Vietnamese tokens = {}'.format( num_vie_tokens))\n",
    "\n",
    "\n",
    "\n",
    "    input_decoder_target_data = list()\n",
    "\n",
    "    # TODO: chúng ta sẽ loại bỏ '<START> ' đầu tiên của các dòng trong biến `tokenized_vie_lines`\n",
    "    # và thêm vào `input_decoder_target_data`\n",
    "\n",
    "\n",
    "    for token_seq in tokenized_train_vie_lines:\n",
    "        input_decoder_target_data.append( token_seq[ 1 : ] )\n",
    "        # input_decoder_target_data.append( token_seq[ 1 : ] )\n",
    "\n",
    "\n",
    "    padded_vie_lines = preprocessing.sequence.pad_sequences(input_decoder_target_data, maxlen=max_output_length, padding='post')\n",
    "    onehot_vie_lines = utils.to_categorical( padded_vie_lines , num_vie_tokens )\n",
    "    decoder_target_data = np.array(onehot_vie_lines)\n",
    "    print( 'Decoder target data shape -> {}'.format( decoder_target_data.shape))\n",
    "\n",
    "    map_vie_i2w = {val: key for key, val in vie_word_dict.items()}\n",
    "    val_target = [[[map_vie_i2w[i] for i in line[1:-1]]] for line in tokenized_val_vie_lines]\n",
    "    return (encoder_train_input_data, decoder_train_input_data, decoder_target_data), (encoder_val_input_data,  decoder_val_input_data, val_target), \\\n",
    "                    (eng_word_dict, vie_word_dict), (num_eng_tokens, num_vie_tokens), (eng_tokenizer, vie_tokenizer), (max_input_length, max_output_length)\n",
    "\n",
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "    # TODO: `angle` sẽ được tính thông qua công thức trên\n",
    "    # với các input là `depth` và `length`\n",
    "\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "    angle = 1 / (10000**depths)         # (1, depth)\n",
    "    angle = positions * angle      # (pos, depth)\n",
    "\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle), np.cos(angle)],\n",
    "        axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    # TODO: định nghĩa lớp `BaseAttention` với input\n",
    "    # là các siêu tham số của 'tf.keras.layers.MultiHeadAttention'\n",
    "\n",
    "    self.mha  = None # MultiHeadAttention\n",
    "    self.layernorm = None\n",
    "    self.add = None\n",
    "\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "  def call(self, x, context):\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        key=context,\n",
    "        value=context,\n",
    "        return_attention_scores=True)\n",
    "\n",
    "    # Cache the attention scores for plotting later.\n",
    "    self.last_attn_scores = attn_scores\n",
    "\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x,\n",
    "        use_causal_mask = True)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "\n",
    "    # TODO: định nghĩa lớp `FeedForward`\n",
    "    # với input là dff, d_model lần lượt là kích thước output của\n",
    "    # lớp 'Dense' thứ nhất và thứ hai, sau lớp `Dense` đầu tiên là\n",
    "    # activation RELU, sau lớp thứ 2 có 1 lớp `Dropout`.\n",
    "    # Sau hai lớp `Dense` là lớp `Add` và `LayerNormalization`.\n",
    "\n",
    "    self.seq = None # nên `tf.keras.Sequential` để chứa 2 lớp `Dense`\n",
    "    self.add = None\n",
    "    self.layer_norm = None\n",
    "    ### BEGIN SOLUTION\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),\n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "    ### END SOLUTION\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x)\n",
    "    return x\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attention = GlobalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.self_attention(x)\n",
    "    x = self.ffn(x)\n",
    "    return x\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads,\n",
    "               dff, vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(\n",
    "        vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "    self.enc_layers = [\n",
    "        EncoderLayer(d_model=d_model,\n",
    "                     num_heads=num_heads,\n",
    "                     dff=dff,\n",
    "                     dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "  def compute_output_shape(self, input_shape):\n",
    "    return [None, None, self.d_model]\n",
    "\n",
    "  def call(self, x):\n",
    "    # `x` is token-IDs shape: (batch, seq_len)\n",
    "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "    # Add dropout.\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x)\n",
    "\n",
    "    return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,\n",
    "               *,\n",
    "               d_model,\n",
    "               num_heads,\n",
    "               dff,\n",
    "               dropout_rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.causal_self_attention = CausalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.cross_attention = CrossAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x, context):\n",
    "    x = self.causal_self_attention(x=x)\n",
    "    x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "    # Cache the last attention scores for plotting later\n",
    "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "    return x\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "               dropout_rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                             d_model=d_model)\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dec_layers = [\n",
    "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                     dff=dff, dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "    self.last_attn_scores = None\n",
    "\n",
    "  def call(self, x, context):\n",
    "    # `x` is token-IDs shape (batch, target_seq_len)\n",
    "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x  = self.dec_layers[i](x, context)\n",
    "\n",
    "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "    return x\n",
    "\n",
    "class Translator:\n",
    "\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.0, vie_word_dict=None, tokenizers=None, loss = masked_loss):\n",
    "        fix_random_seed()\n",
    "        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=input_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=target_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size, activation=tf.keras.activations.softmax)\n",
    "\n",
    "        self.eng_tokenizer = tokenizers[0]\n",
    "        self.vie_tokenizer = tokenizers[1]\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.map_vie_i2w = {val: key for key, val in vie_word_dict.items()}\n",
    "        self.vie_word_dict = vie_word_dict\n",
    "        self.loss = loss\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        encoder_inputs = tf.keras.layers.Input(shape=( None,))\n",
    "        decoder_inputs = tf.keras.layers.Input(shape=( None,))\n",
    "\n",
    "        context = self.encoder(encoder_inputs)  # (batch_size, context_len, d_model)\n",
    "\n",
    "        output = self.decoder(decoder_inputs, context)  # (batch_size, target_len, d_model)\n",
    "\n",
    "        # Final linear layer output.\n",
    "        output = self.final_layer(output)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "        try:\n",
    "            # Delete the keras mask, so keras doesn't scale the loss+accuracy.\n",
    "            del output._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        self.model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "\n",
    "\n",
    "    # Load mô hình từ file\n",
    "    def load(self, model_file):\n",
    "        self.model = load_model(model_file)\n",
    "\n",
    "    # Lưu mô hình hiện tại xuống file\n",
    "    def save(self, model_file):\n",
    "        self.model.save(model_file)\n",
    "\n",
    "    # Tóm tắt kiến trúc mạng\n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    # Thử nghiệm mô hình với dữ liệu ảnh đầu vào\n",
    "    def predict(self, x_test):\n",
    "        return self.model.predict(x_test)\n",
    "\n",
    "    def train(self, encoder_input_data , decoder_input_data, decoder_target_data):\n",
    "        # Các hyper-parameter ở đây được chỉnh để có thể so sánh với mô hình trước.\n",
    "        # Nếu muốn, các bạn có thể tinh chỉnh để đạt hiệu suất cao hơn.\n",
    "        learning_rate = CustomSchedule(self.d_model)\n",
    "        # print(learning_rate)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                        epsilon=1e-9)\n",
    "        self.model.compile(\n",
    "                loss=self.loss,\n",
    "                optimizer=optimizer,\n",
    "                metrics=[masked_accuracy],\n",
    "                )\n",
    "        self.model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=8, epochs=12)\n",
    "\n",
    "    def translate(self, input, return_attention=False):\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        input = tf.convert_to_tensor(input, dtype=tf.int64)\n",
    "\n",
    "        start, end = self.vie_word_dict['start'], self.vie_word_dict['end']\n",
    "        output_array = output_array.write(0, [start])\n",
    "\n",
    "        for i in tf.range(max_output_length):\n",
    "            output = tf.transpose(output_array.stack())\n",
    "            predictions = self.model([input, output], training=False)\n",
    "\n",
    "            # Select the last token from the `seq_len` dimension.\n",
    "\n",
    "            predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "            predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "            # Concatenate the `predicted_id` to the output which is given to the\n",
    "            if predicted_id[0] == end or predicted_id[0]==0:\n",
    "                break\n",
    "\n",
    "            output_array = output_array.write(i+1, predicted_id[0])\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        # print(output)\n",
    "        text = [self.map_vie_i2w.get(i, '') for i in output[0].numpy()[1:]]  # Shape: `()`.\n",
    "        if return_attention:\n",
    "            self.model([input, output], training=False)\n",
    "            attention_weights = self.decoder.last_attn_scores\n",
    "            attention_heads = tf.squeeze(attention_weights, 0)\n",
    "            return text, attention_heads\n",
    "        return text\n",
    "\n",
    "    def plot_attention(self, text, **kwargs):\n",
    "        assert isinstance(text, str)\n",
    "\n",
    "        input = self.eng_tokenizer.texts_to_sequences([text])\n",
    "        input = preprocessing.sequence.pad_sequences(input, maxlen=max_input_length , padding='post' )\n",
    "        output, attention = self.translate(input, return_attention=True)\n",
    "        attention_weights = tf.concat(attention, 0)\n",
    "        context = text.split()\n",
    "\n",
    "        for i in range(len(attention_weights)):\n",
    "            attention = attention_weights[i][:, :len(context)]\n",
    "            fig = plt.figure(figsize=(10, 10))\n",
    "            ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "            ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
    "\n",
    "            fontdict = {'fontsize': 14}\n",
    "\n",
    "            ax.set_xticklabels([''] + context, fontdict=fontdict, rotation=90)\n",
    "            ax.set_yticklabels([''] + output, fontdict=fontdict)\n",
    "\n",
    "            ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "            ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "            ax.set_ylabel('Output text')\n",
    "            ax.set_xlabel(f'Head {i}')\n",
    "\n",
    "\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "def evaluate(model, encoder_val_input_data, val_target, num_sample=None):\n",
    "    num_sample = encoder_val_input_data.shape[0] if num_sample==None else num_sample\n",
    "\n",
    "    predict_translation = []\n",
    "    eng_sentence = []\n",
    "\n",
    "    for i in tqdm(range( num_sample ) ):\n",
    "        decoded_translation = model.translate(encoder_val_input_data[ i ][None,...])\n",
    "        eng_sentence.append(encoder_val_input_data[ i ])\n",
    "        predict_translation.append( decoded_translation )\n",
    "\n",
    "\n",
    "    predict = [f for f in predict_translation]\n",
    "\n",
    "    references = val_target[:num_sample]\n",
    "    candidates = predict\n",
    "    score = corpus_bleu(references, candidates, weights=(0.5, 0.5))\n",
    "    print('\\nGiá trị bleu score là', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dde74ef-5263-4624-8f4c-f1965c2a3842",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T00:55:56.394374Z",
     "iopub.status.busy": "2025-04-11T00:55:56.393894Z",
     "iopub.status.idle": "2025-04-11T00:55:56.398642Z",
     "shell.execute_reply": "2025-04-11T00:55:56.397937Z",
     "shell.execute_reply.started": "2025-04-11T00:55:56.394355Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import các thư viện cần thiết\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.models import load_model, Model\n",
    "from keras import layers , activations , models , preprocessing , utils\n",
    "from tensorflow.keras import layers, activations, models, preprocessing, utils\n",
    "fix_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ac73f0-a4f0-4fad-a7a8-e2ec5dd0acb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T00:55:56.399604Z",
     "iopub.status.busy": "2025-04-11T00:55:56.399361Z",
     "iopub.status.idle": "2025-04-11T00:55:56.666378Z",
     "shell.execute_reply": "2025-04-11T00:55:56.665642Z",
     "shell.execute_reply.started": "2025-04-11T00:55:56.399578Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>vie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Who's your favorite movie star?</td>\n",
       "      <td>Ngôi sao điện ảnh mà bạn yêu thích là ai thế?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Who's your favorite movie star?</td>\n",
       "      <td>Ngôi sao điện ảnh mà bạn yêu thích là ai vậy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Why aren't you in your uniform?</td>\n",
       "      <td>Tại sao bạn không mặc đồng phục?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Will he be coming this evening?</td>\n",
       "      <td>Tối nay anh ấy có đến không?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Will he be coming this evening?</td>\n",
       "      <td>Tối nay ông ấy có đến không?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  eng  \\\n",
       "4995  Who's your favorite movie star?   \n",
       "4996  Who's your favorite movie star?   \n",
       "4997  Why aren't you in your uniform?   \n",
       "4998  Will he be coming this evening?   \n",
       "4999  Will he be coming this evening?   \n",
       "\n",
       "                                                vie  \n",
       "4995  Ngôi sao điện ảnh mà bạn yêu thích là ai thế?  \n",
       "4996  Ngôi sao điện ảnh mà bạn yêu thích là ai vậy?  \n",
       "4997               Tại sao bạn không mặc đồng phục?  \n",
       "4998                   Tối nay anh ấy có đến không?  \n",
       "4999                   Tối nay ông ấy có đến không?  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = None\n",
    "lines = pd.read_table('./data/vie.txt' , names=['eng' , 'vie' , 'c' ] )\n",
    "lines = lines.drop(['c'] , axis=1 )[0:5000]\n",
    "lines.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a771b6d2-0c2d-4df6-adf4-5b48e2332646",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T00:55:56.668034Z",
     "iopub.status.busy": "2025-04-11T00:55:56.667769Z",
     "iopub.status.idle": "2025-04-11T00:55:58.096932Z",
     "shell.execute_reply": "2025-04-11T00:55:58.096181Z",
     "shell.execute_reply.started": "2025-04-11T00:55:56.668018Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Độ dài lớn nhất của English là 9\n",
      "Kích thước dữ liệu của Encoder  -> (4500, 17)\n",
      "Số lượng English tokens = 2451\n",
      "Độ dài lớn nhất của tiếng việt là 17\n",
      "kích thước dữ liệu đầu vào của Decoder -> (4500, 17)\n",
      "Số lượng Vietnamese tokens = 1861\n",
      "Decoder target data shape -> (4500, 17, 1861)\n"
     ]
    }
   ],
   "source": [
    "# from utils import data_preprocessing\n",
    "train_data, val_data, word_dict, num_tokens, tokenizers, max_length = data_preprocessing(lines)\n",
    "\n",
    "(encoder_train_input_data, decoder_train_input_data, decoder_target_data) = train_data\n",
    "(encoder_val_input_data,  decoder_val_input_data, val_target) = val_data\n",
    "(eng_word_dict, vie_word_dict), (num_eng_tokens, num_vie_tokens) = word_dict, num_tokens\n",
    "(eng_tokenizer, vie_tokenizer), (max_input_length, max_output_length) = tokenizers, max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22139bc7-9fea-441e-adff-f8b89ee02317",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T00:55:58.098288Z",
     "iopub.status.busy": "2025-04-11T00:55:58.097754Z",
     "iopub.status.idle": "2025-04-11T00:55:58.101720Z",
     "shell.execute_reply": "2025-04-11T00:55:58.101057Z",
     "shell.execute_reply.started": "2025-04-11T00:55:58.098262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "d_model = 64\n",
    "dff = 128\n",
    "num_heads = 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0f702b-94c2-4332-aeca-732a971d6636",
   "metadata": {},
   "source": [
    "## Khởi tạo và huấn luyện mô hình với cài đặt mặc định"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17672cfe-6979-40f1-974d-fc3d24c63f85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T00:55:58.102849Z",
     "iopub.status.busy": "2025-04-11T00:55:58.102603Z",
     "iopub.status.idle": "2025-04-11T01:03:13.716414Z",
     "shell.execute_reply": "2025-04-11T01:03:13.715758Z",
     "shell.execute_reply.started": "2025-04-11T00:55:58.102832Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744332959.189414      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'global_self_attention' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'feed_forward' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'encoder_layer' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'global_self_attention_1' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_1' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'feed_forward_1' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'encoder_layer_1' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'causal_self_attention' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'cross_attention' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_2' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'feed_forward_2' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'decoder_layer' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'causal_self_attention_1' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'cross_attention_1' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_3' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'feed_forward_3' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'decoder_layer_1' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744332984.465885      88 service.cc:148] XLA service 0x7ce02800d6f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1744332984.466470      88 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1744332986.370313      88 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 27/563\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 7.5773 - masked_accuracy: 3.4348e-04   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744332995.011981      88 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 24ms/step - loss: 7.0444 - masked_accuracy: 0.0774\n",
      "Epoch 2/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.0815 - masked_accuracy: 0.2100\n",
      "Epoch 3/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.3011 - masked_accuracy: 0.3039\n",
      "Epoch 4/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.7245 - masked_accuracy: 0.3765\n",
      "Epoch 5/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.2703 - masked_accuracy: 0.4279\n",
      "Epoch 6/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.9025 - masked_accuracy: 0.4771\n",
      "Epoch 7/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.6382 - masked_accuracy: 0.5045\n",
      "Epoch 8/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.4101 - masked_accuracy: 0.5338\n",
      "Epoch 9/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.1217 - masked_accuracy: 0.5731\n",
      "Epoch 10/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1.8933 - masked_accuracy: 0.6074\n",
      "Epoch 11/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1.7139 - masked_accuracy: 0.6359\n",
      "Epoch 12/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1.5713 - masked_accuracy: 0.6589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 2, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n",
      "100%|██████████| 500/500 [05:49<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Giá trị bleu score là 0.3868717220183509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "alpha=Translator(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=num_eng_tokens,\n",
    "    target_vocab_size=num_vie_tokens,\n",
    "    dropout_rate=dropout_rate,\n",
    "    vie_word_dict=vie_word_dict, tokenizers=tokenizers)\n",
    "\n",
    "alpha.build()\n",
    "\n",
    "alpha.train(encoder_train_input_data , decoder_train_input_data, decoder_target_data)\n",
    "evaluate(alpha, encoder_val_input_data, val_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa8d62c-64ba-4b07-b5f5-7c7074b696d8",
   "metadata": {},
   "source": [
    "## Thay Đổi Feedforward Network (FFN) trong Transformer\n",
    "\n",
    "\n",
    "Trong mô hình Transformer, một trong những thành phần quan trọng là FeedForward (FFN). Khối này bao gồm hai lớp Dense và một hàm kích hoạt (thường là ReLU) giữa chúng, cùng với một skip connection để cải thiện khả năng học của mô hình. Vậy nếu chúng ta thay đổi cấu trúc của FFN, cụ thể là loại bỏ hàm kích hoạt ReLU hoặc loại bỏ skip connection thì điều gì sẽ xảy ra?\n",
    "\n",
    "Trong phần thực nghiệm này, chúng ta sẽ tiến hành các thay đổi sau đối với FFN:\n",
    "  - Loại bỏ ReLU:\n",
    "  - Loại bỏ Skip Connection:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a434eb-5e46-4718-afe0-1e06682d8618",
   "metadata": {},
   "source": [
    "### Loại bỏ ReLU\n",
    "Thông thường, khối FFN trong Transformer sử dụng hàm kích hoạt ReLU giữa hai lớp Dense. ReLU giúp mô hình học được các đặc trưng phi tuyến tính và cải thiện hiệu suất của mô hình. Tuy nhiên, nếu chúng ta loại bỏ ReLU, khối FFN sẽ trở thành một khối tuyến tính thuần túy. Điều này có thể làm giảm khả năng học các đặc trưng phức tạp của mô hình, và chúng ta sẽ kiểm tra xem hiệu suất của mô hình thay đổi như thế nào."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60dbe828-ae6d-4fc2-b5d1-2b45ca00c059",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:03:13.717580Z",
     "iopub.status.busy": "2025-04-11T01:03:13.717301Z",
     "iopub.status.idle": "2025-04-11T01:03:13.723186Z",
     "shell.execute_reply": "2025-04-11T01:03:13.722512Z",
     "shell.execute_reply.started": "2025-04-11T01:03:13.717555Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DFeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.dff = dff\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # TODO: định nghĩa hàm `__init__` sao cho hai lớp Fully Connected sử dụng các hàm kích hoạt tuyến tính\n",
    "        #\n",
    "        # Lưu ý: Vẫn giữ drop-out như kiến trúc cũ\n",
    "\n",
    "        ### BEGIN SOLUTION\n",
    "        self.seq = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(dff),\n",
    "          tf.keras.layers.Dense(d_model),\n",
    "          tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        ### END SOLUTION\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d2ca128-81ad-4d1e-bd1e-bc6c6a283a29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:03:13.724257Z",
     "iopub.status.busy": "2025-04-11T01:03:13.723989Z",
     "iopub.status.idle": "2025-04-11T01:09:59.955212Z",
     "shell.execute_reply": "2025-04-11T01:09:59.954518Z",
     "shell.execute_reply.started": "2025-04-11T01:03:13.724234Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'global_self_attention_6' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_11' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'd_feed_forward' (of type DFeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'd_encoder_layer', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'd_encoder_layer' (of type DEncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'global_self_attention_7' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_13' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'd_feed_forward_1' (of type DFeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'd_encoder_layer_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'd_encoder_layer_1' (of type DEncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'd_encoder', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'causal_self_attention_6' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'cross_attention_6' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_17' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'd_feed_forward_2' (of type DFeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'd_decoder_layer', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'd_decoder_layer' (of type DDecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'causal_self_attention_7' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'cross_attention_7' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_19' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'd_feed_forward_3' (of type DFeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'd_decoder_layer_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'd_decoder_layer_1' (of type DDecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'd_decoder', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 23ms/step - loss: 7.0272 - masked_accuracy: 0.0779\n",
      "Epoch 2/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.0942 - masked_accuracy: 0.2060\n",
      "Epoch 3/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.3220 - masked_accuracy: 0.2971\n",
      "Epoch 4/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.8014 - masked_accuracy: 0.3636\n",
      "Epoch 5/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.4181 - masked_accuracy: 0.4097\n",
      "Epoch 6/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.1211 - masked_accuracy: 0.4433\n",
      "Epoch 7/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.9208 - masked_accuracy: 0.4635\n",
      "Epoch 8/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.7754 - masked_accuracy: 0.4828\n",
      "Epoch 9/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.5558 - masked_accuracy: 0.5063\n",
      "Epoch 10/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.3616 - masked_accuracy: 0.5362\n",
      "Epoch 11/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.2127 - masked_accuracy: 0.5561\n",
      "Epoch 12/12\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.0823 - masked_accuracy: 0.5764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [05:24<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Giá trị bleu score là 0.2959953839324422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class DEncoderLayer(EncoderLayer):\n",
    "  def __init__(self,**kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.ffn = DFeedForward(d_model, dff)\n",
    "\n",
    "\n",
    "class DEncoder(Encoder):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "\n",
    "    self.enc_layers = [\n",
    "        DEncoderLayer(d_model=d_model,\n",
    "                     num_heads=num_heads,\n",
    "                     dff=dff,\n",
    "                     dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "class DDecoderLayer(DecoderLayer):\n",
    "  def __init__(self,\n",
    "               **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.ffn = DFeedForward(d_model, dff)\n",
    "\n",
    "\n",
    "\n",
    "class DDecoder(Decoder):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "\n",
    "    self.dec_layers = [\n",
    "        DDecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                     dff=dff, dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "\n",
    "class Delta(Translator):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.0, vie_word_dict=None, tokenizers=None):\n",
    "        super().__init__(num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate, vie_word_dict, tokenizers)\n",
    "        self.encoder = DEncoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=input_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "        self.decoder = DDecoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=target_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "\n",
    "delta=Delta(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=num_eng_tokens,\n",
    "    target_vocab_size=num_vie_tokens,\n",
    "    dropout_rate=dropout_rate,\n",
    "    vie_word_dict=vie_word_dict,\n",
    "    tokenizers=tokenizers)\n",
    "\n",
    "\n",
    "delta.build()\n",
    "\n",
    "delta.train(encoder_train_input_data , decoder_train_input_data, decoder_target_data)\n",
    "evaluate(delta, encoder_val_input_data, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed5fc096-23e2-49e3-93de-70114d345d2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:09:59.956182Z",
     "iopub.status.busy": "2025-04-11T01:09:59.955970Z",
     "iopub.status.idle": "2025-04-11T01:10:00.091810Z",
     "shell.execute_reply": "2025-04-11T01:10:00.091019Z",
     "shell.execute_reply.started": "2025-04-11T01:09:59.956164Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### BEGIN PUBLIC TESTS\n",
    "x = tf.random.uniform((1, 10, 64))\n",
    "\n",
    "custom_ffn = DFeedForward(64,128)\n",
    "ffn = FeedForward(64,128)\n",
    "\n",
    "out_ffn = ffn(x)\n",
    "out_custom_ffn = custom_ffn(x)\n",
    "\n",
    "assert not tf.reduce_all(tf.equal(out_ffn, out_custom_ffn))\n",
    "### END PUBLIC TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "066096db-0aa7-4a71-8a1d-e6449137e160",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:10:00.094334Z",
     "iopub.status.busy": "2025-04-11T01:10:00.094112Z",
     "iopub.status.idle": "2025-04-11T01:10:00.099440Z",
     "shell.execute_reply": "2025-04-11T01:10:00.098698Z",
     "shell.execute_reply.started": "2025-04-11T01:10:00.094317Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theo mình thì:  hiệu suất beta xấp xỉ alpha (không chêch lệch quá 1%)\n"
     ]
    }
   ],
   "source": [
    "options = {0: 'hiệu suất beta xấp xỉ alpha (không chêch lệch quá 1%)',\n",
    "           1: 'hiệu suất beta thấp hơn alpha k% (1<k<5)',\n",
    "           2: 'hiệu suất beta thấp hơn alpha k% (5<=k<10)',\n",
    "           3: 'hiệu suất beta thấp hơn alpha k% (10<=k<20)',\n",
    "           4: 'hiệu suất beta thấp hơn alpha trên 20%',\n",
    "           5: 'beta gần như không học gì (hiệu suất thấp hơn 10%)'}\n",
    "your_choice = None\n",
    "### BEGIN SOLUTION\n",
    "your_choice = 2\n",
    "### END SOLUTION\n",
    "print(\"Theo mình thì: \", options[your_choice])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8a281-358e-4215-a931-e93a0d607f07",
   "metadata": {},
   "source": [
    "### Loại bỏ Skip connection trong FFN\n",
    "Skip connection là một thành phần quan trọng trong Transformer, giúp truyền thông tin từ đầu vào trực tiếp đến đầu ra của khối FFN mà không qua các lớp trung gian. Điều này giúp giảm thiểu vấn đề gradient vanishing và giúp mô hình học nhanh hơn và ổn định hơn. Khi loại bỏ skip connection, chúng ta sẽ buộc mô hình học mọi đặc trưng chỉ thông qua các lớp Dense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab34174-a59d-4814-b494-479446091544",
   "metadata": {},
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b946dd69-9b14-4cd9-a472-add4e1219f18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:24:02.682839Z",
     "iopub.status.busy": "2025-04-11T01:24:02.682456Z",
     "iopub.status.idle": "2025-04-11T01:24:02.688626Z",
     "shell.execute_reply": "2025-04-11T01:24:02.687796Z",
     "shell.execute_reply.started": "2025-04-11T01:24:02.682817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: định nghĩa lớp `EFeedForward` có kiến trúc tương tự như `FeedForward` trước đây nhưng không sử dụng skip-connection\n",
    "# Lưu ý: Loại bỏ các khởi tạo không dùng đến\n",
    "class EFeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        ### BEGIN SOLUTION\n",
    "        self.seq = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(dff, activation='relu'),\n",
    "          tf.keras.layers.Dense(d_model),\n",
    "          tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def call(self, x):\n",
    "        ### BEGIN SOLUTION\n",
    "        x = self.layer_norm(self.seq(x))\n",
    "        return x\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fab808-f20a-4a63-96cb-86ef07f8c5f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:24:02.690044Z",
     "iopub.status.busy": "2025-04-11T01:24:02.689835Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'global_self_attention_18' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_45' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'e_feed_forward_4' (of type EFeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'e_encoder_layer_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'e_encoder_layer_2' (of type EEncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'global_self_attention_19' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_47' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'e_feed_forward_5' (of type EFeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'e_encoder_layer_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'e_encoder_layer_3' (of type EEncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'e_encoder_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'causal_self_attention_18' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'cross_attention_18' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_51' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'e_feed_forward_6' (of type EFeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'e_decoder_layer_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'e_decoder_layer_2' (of type EDecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'causal_self_attention_19' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'cross_attention_19' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_53' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'e_feed_forward_7' (of type EFeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'e_decoder_layer_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'e_decoder_layer_3' (of type EDecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'e_decoder_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n"
     ]
    }
   ],
   "source": [
    "class EEncoderLayer(EncoderLayer):\n",
    "  def __init__(self,**kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.ffn = EFeedForward(d_model, dff)\n",
    "\n",
    "\n",
    "class EEncoder(Encoder):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "\n",
    "    self.enc_layers = [\n",
    "        EEncoderLayer(d_model=d_model,\n",
    "                     num_heads=num_heads,\n",
    "                     dff=dff,\n",
    "                     dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "class EDecoderLayer(DecoderLayer):\n",
    "  def __init__(self,\n",
    "               **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.ffn = EFeedForward(d_model, dff)\n",
    "\n",
    "\n",
    "\n",
    "class EDecoder(Decoder):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "\n",
    "    self.dec_layers = [\n",
    "        EDecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                     dff=dff, dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "\n",
    "class Epsilon(Translator):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.0, vie_word_dict=None, tokenizers=None):\n",
    "        super().__init__(num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate, vie_word_dict, tokenizers)\n",
    "        self.encoder = EEncoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=input_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "        self.decoder = EDecoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=target_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "epsilon=Epsilon(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=num_eng_tokens,\n",
    "    target_vocab_size=num_vie_tokens,\n",
    "    dropout_rate=dropout_rate,\n",
    "    vie_word_dict=vie_word_dict,\n",
    "    tokenizers=tokenizers)\n",
    "\n",
    "\n",
    "epsilon.build()\n",
    "\n",
    "epsilon.train(encoder_train_input_data , decoder_train_input_data, decoder_target_data)\n",
    "evaluate(epsilon, encoder_val_input_data, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4c2873-5c45-452b-923c-0bc29ea5193d",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "try:\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    x = tf.random.uniform((1, 10, 64))\n",
    "\n",
    "    custom_ffn = EFeedForward(64,128)\n",
    "    ffn = FeedForward(64,128)\n",
    "\n",
    "    out_ffn = ffn(x)\n",
    "    out_custom_ffn = custom_ffn(x)\n",
    "\n",
    "    assert not tf.reduce_all(tf.equal(out_ffn, out_custom_ffn))\n",
    "### END PUBLIC TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1920e5-fb1b-435f-9355-9330612edc83",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "options = {0: 'hiệu suất beta xấp xỉ alpha (không chêch lệch quá 1%)',\n",
    "           1: 'hiệu suất beta thấp hơn alpha k% (1<k<5)',\n",
    "           2: 'hiệu suất beta thấp hơn alpha k% (5<=k<10)',\n",
    "           3: 'hiệu suất beta thấp hơn alpha k% (10<=k<20)',\n",
    "           4: 'hiệu suất beta thấp hơn alpha trên 20%',\n",
    "           5: 'beta gần như không học gì (hiệu suất thấp hơn 10%)'}\n",
    "your_choice = None\n",
    "### BEGIN SOLUTION\n",
    "your_choice = 3\n",
    "### END SOLUTION\n",
    "print(\"Theo mình thì: \", options[your_choice])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7110550,
     "sourceId": 11360884,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
