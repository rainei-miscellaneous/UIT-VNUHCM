{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1293,
     "status": "ok",
     "timestamp": 1711646935952,
     "user": {
      "displayName": "Nguyễn Đức Vũ",
      "userId": "07288779277842550568"
     },
     "user_tz": -420
    },
    "id": "48UH6zEyaSc8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import cosine\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1711646804012,
     "user": {
      "displayName": "Nguyễn Đức Vũ",
      "userId": "07288779277842550568"
     },
     "user_tz": -420
    },
    "id": "ZVdb62fRyhTn"
   },
   "outputs": [],
   "source": [
    "def load_vocab_dict(path: str) -> Tuple[List[str], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Reads a vocabulary list from a file and creates a dictionary mapping each word to its index.\n",
    "\n",
    "    Args:\n",
    "        path (str): The file path to the vocabulary list.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], Dict[str, int]]: A tuple containing the list of vocabulary words and a dictionary\n",
    "                                          mapping each word to its index.\n",
    "    \"\"\"\n",
    "    vocab = open(path).read().strip().split('\\n')\n",
    "    return vocab, {word: idx for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1711646804013,
     "user": {
      "displayName": "Nguyễn Đức Vũ",
      "userId": "07288779277842550568"
     },
     "user_tz": -420
    },
    "id": "hkJ7fQifz532"
   },
   "outputs": [],
   "source": [
    "def read_corpus(path: str) -> List[str]:\n",
    "    \"\"\"Reads the corpus from a file, excluding the last empty entry if the file ends with a newline.\n",
    "\n",
    "    Args:\n",
    "        path (str): The file path to the corpus.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of strings, each representing a line from the file.\n",
    "    \"\"\"\n",
    "    return open(path).read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1711646804013,
     "user": {
      "displayName": "Nguyễn Đức Vũ",
      "userId": "07288779277842550568"
     },
     "user_tz": -420
    },
    "id": "_cgReXf9FIl2"
   },
   "outputs": [],
   "source": [
    "def counting(corpus: List[str], V: List[str], V_C: List[str], V_set: Dict[str, int], V_C_set: Dict[str, int], w: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates a co-occurrence (counting) matrix from the given corpus, considering specified vocabularies and a window size.\n",
    "\n",
    "    Args:\n",
    "        corpus (List[str]): The corpus as a list of sentences.\n",
    "        V (List[str]): The list of vocabulary words.\n",
    "        V_C (List[str]): The list of context vocabulary words.\n",
    "        V_set (Dict[str, int]): A dictionary mapping vocabulary words to their indices.\n",
    "        V_C_set (Dict[str, int]): A dictionary mapping context vocabulary words to their indices.\n",
    "        w (int): The window size for context.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 2D NumPy array representing the co-occurrence matrix with dimensions (len(V), len(V_C)).\n",
    "    \"\"\"\n",
    "    # Initialize the matrix to hold word vectors\n",
    "    C = np.zeros((len(V), len(V_C)), dtype=float)\n",
    "\n",
    "    for line in tqdm(corpus): # Iterate over each word in the original dataset\n",
    "        # Append start and end tokens to the sentence\n",
    "        words = ['<s>'] + line.split(' ') + ['</s>']\n",
    "        length = len(words)\n",
    "\n",
    "        for idx, word in enumerate(words): # Iterate over each word in the current sentence\n",
    "            # Skip '<s>' and '</s>', as they are not real words\n",
    "            if idx > 0 and idx < length - 1 and word in V_set:\n",
    "                # Iterate over left and right context words within the window w\n",
    "                context_words = words[max(idx-w,0):idx] + words[idx+1:min(idx+w+1,length)]\n",
    "\n",
    "                # Constructs a co-occurrence matrix by iterating over context words\n",
    "                # within a specified range and increments counts in the matrix\n",
    "                # for each word-context pair found in a predefined vocabulary.\n",
    "                # It quantifies the relationship between words and their context in a corpus,\n",
    "                # essential for analyzing word associations.\n",
    "\n",
    "                ### BEGIN SOLUTION\n",
    "                for cWord in context_words:\n",
    "                    if cWord in V_C_set:\n",
    "                        C[V_set[word], V_C_set[cWord]] += 1\n",
    "                ### END SOLUTION\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1711646943804,
     "user": {
      "displayName": "Nguyễn Đức Vũ",
      "userId": "07288779277842550568"
     },
     "user_tz": -420
    },
    "id": "_KAL4m9SDyKA"
   },
   "outputs": [],
   "source": [
    "def eval_word_similarity(C: np.ndarray, V_set: Dict[str, int], path: str) -> float:\n",
    "    \"\"\"\n",
    "    Evaluates word similarity by comparing a calculated similarity matrix against a gold standard dataset.\n",
    "\n",
    "    Args:\n",
    "        C (np.ndarray): A 2D NumPy array where rows represent words and columns represent their vector embeddings.\n",
    "        V_set (Dict[str, int]): A dictionary mapping words to their indices in the matrix C.\n",
    "        path (str): The file path to the gold standard dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: The Spearman correlation coefficient between the gold standard similarity scores and the calculated scores.\n",
    "    \"\"\"\n",
    "    # Read the gold standard data, skipping the header and the last empty line if present\n",
    "    gold = [line.split('\\t') for line in open(path).read().strip().split('\\n')[1:]]\n",
    "\n",
    "    # Prepare gold scores and similarity scores\n",
    "    y = [float(line[2]) for line in gold]  # Extract gold standard similarity scores\n",
    "    x = [\n",
    "        1 - cosine(C[V_set[word_1], :], C[V_set[word_2], :]) if word_1 in V_set and word_2 in V_set else 0\n",
    "        for word_1, word_2, _ in gold\n",
    "    ]\n",
    "\n",
    "    # Calculate and return Spearman correlation\n",
    "    return stats.spearmanr(x, y, axis=None).correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114004,
     "status": "ok",
     "timestamp": 1711646918013,
     "user": {
      "displayName": "Nguyễn Đức Vũ",
      "userId": "07288779277842550568"
     },
     "user_tz": -420
    },
    "id": "tKH1dmT2D8V4",
    "outputId": "cbca5f46-8a0f-4da5-9a89-51835eb1dcf8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997898/997898 [01:46<00:00, 9388.66it/s] \n"
     ]
    }
   ],
   "source": [
    "# Read the main vocabulary and its indices from a file,\n",
    "# creating a list of words (V) and a dictionary mapping words to indices (V_set).\n",
    "V, V_set = load_vocab_dict('./data/main_words.txt')\n",
    "\n",
    "# Read the context vocabulary and its indices from a separate file,\n",
    "# creating a list of context words (V_C) and a dictionary mapping these words to indices (V_C_set).\n",
    "V_C, V_C_set = load_vocab_dict('./data/context_words.txt')\n",
    "\n",
    "# Read the corpus from a text file, creating a list where each item represents a document or line in the corpus.\n",
    "corpus = read_corpus('./data/corpus.txt')\n",
    "\n",
    "# Generate a co-occurrence (counting) matrix from the corpus using the main and context vocabularies.\n",
    "# The window size 'w=3' indicates the context range around each target word to consider for co-occurrences.\n",
    "C = counting(corpus, V, V_C, V_set, V_C_set, w=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1711646948109,
     "user": {
      "displayName": "Nguyễn Đức Vũ",
      "userId": "07288779277842550568"
     },
     "user_tz": -420
    },
    "id": "3VvhpytXBs1A",
    "outputId": "24707dd3-b3ed-472d-8b03-52fee9edd664"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23223229343659896"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_word_similarity(C, V_set, './data/men.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1711646949884,
     "user": {
      "displayName": "Nguyễn Đức Vũ",
      "userId": "07288779277842550568"
     },
     "user_tz": -420
    },
    "id": "YNde13fFBxMz",
    "outputId": "f8b94b4c-a48d-4102-9f79-9d398fe9a9bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\Lib\\site-packages\\scipy\\spatial\\distance.py:647: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  dist = 1.0 - uv / math.sqrt(uu * vv)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_word_similarity(C, V_set, './data/simlex-999.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 22862,
     "status": "ok",
     "timestamp": 1711646974553,
     "user": {
      "displayName": "Nguyễn Đức Vũ",
      "userId": "07288779277842550568"
     },
     "user_tz": -420
    },
    "id": "vAn1zXmOE6zT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_11228\\3373784493.py:30: RuntimeWarning: divide by zero encountered in log2\n",
      "  pmi_matrix = np.log2(p_xy / (np.outer(p_x, p_y) + 6.67e-9))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean(T1, T2): 0.4362232747857433\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\"\"\"Bai lam co tham khao ChatGPT va ban be\"\"\"\n",
    "\n",
    "def improve_C_with_grid_search(C: np.ndarray, grid: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Improves the input co-occurrence matrix C using Positive Pointwise Mutual Information (PPMI)\n",
    "    and applies PCA using grid search to find the best n_components.\n",
    "\n",
    "    Args:\n",
    "        C (np.ndarray): The co-occurrence matrix with shape (len(V), len(V_C)), where len(V) is the number of\n",
    "                        vocabulary words, and len(V_C) is the number of context words.\n",
    "        grid (list): A list of n_components to try in PCA.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The best C_improved matrix and the best n_components.\n",
    "    \"\"\"\n",
    "    np.random.seed(710)\n",
    "    total = np.sum(C)\n",
    "    word_count = np.sum(C, axis=1)\n",
    "    context_count = np.sum(C, axis=0)\n",
    "\n",
    "    p_xy = C / total\n",
    "    p_x = word_count / total\n",
    "    p_y = context_count / total\n",
    "\n",
    "    pmi_matrix = np.log2(p_xy / (np.outer(p_x, p_y) + 6.67e-9))\n",
    "    pmi_matrix[p_xy == 0] = 0\n",
    "    pmi_matrix[np.isnan(pmi_matrix)] = 0\n",
    "\n",
    "    PPMI = np.maximum(pmi_matrix, 0)\n",
    "\n",
    "    NPMI = (PPMI - np.log2(p_y)) / - np.log2(p_xy.shape[0])\n",
    "    NPMI[np.isnan(NPMI)] = 0\n",
    "\n",
    "    C_improved = NPMI / np.linalg.norm(NPMI, axis=1, keepdims=True)\n",
    "    C_improved = StandardScaler().fit_transform(C_improved)\n",
    "\n",
    "    best_n_components = None\n",
    "    best_score = -np.inf\n",
    "    best_C_improved = None\n",
    "\n",
    "    # Grid search\n",
    "    for n_components in grid:\n",
    "        pca = PCA(n_components=n_components)\n",
    "        C_reduced = pca.fit_transform(C_improved)\n",
    "\n",
    "        T1 = eval_word_similarity(C_reduced, V_set, './data/men.txt')\n",
    "        T2 = eval_word_similarity(C_reduced, V_set, './data/simlex-999.txt')\n",
    "        mean_score = np.mean([T1, T2])\n",
    "\n",
    "        # print(f\"n_components: {n_components}, Mean(T1, T2): {mean_score}\")\n",
    "\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_n_components = n_components\n",
    "            best_C_improved = C_reduced\n",
    "\n",
    "    # print(f\"Best n_components: {best_n_components}, Best Mean(T1, T2): {best_score}\")\n",
    "    return best_C_improved, best_n_components\n",
    "\n",
    "grid = [1050]\n",
    "\n",
    "C_improved, best_n_components = improve_C_with_grid_search(C, grid)\n",
    "\n",
    "C_improved = C_improved - 0.2\n",
    "\n",
    "T1 = eval_word_similarity(C_improved, V_set, './data/men.txt')\n",
    "T2 = eval_word_similarity(C_improved, V_set, './data/simlex-999.txt')\n",
    "mean_score = np.mean([T1, T2])\n",
    "\n",
    "print(f\"Mean(T1, T2): {mean_score}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
