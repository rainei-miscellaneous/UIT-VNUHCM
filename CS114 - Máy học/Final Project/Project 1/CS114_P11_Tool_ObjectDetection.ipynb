{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SỬ DỤNG OBJECT DETECTION ĐỂ CROP ẢNH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Thông tin của tác giả, ngày cập nhật**\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Thành viên nhóm**:\n",
    "- **Trần Đình Khánh Đăng - 22520195**\n",
    "- **Tăng Nhất - 22521027**\n",
    "- **Lê Minh Nhựt - 22521060**\n",
    "\n",
    "**Ngày cập nhật**: 22/01/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Khởi tạo đường dẫn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '/kaggle/working/dataset'\n",
    "base_dir = '/kaggle/input/cs114-final-project-full-dataset'\n",
    "df = pd.read_csv('CarDataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1\"\n",
    "detector = hub.load(model_url)\n",
    "\n",
    "def save_cropped_image(cropped_image, original_image_path, dataset_dir='./'):\n",
    "    base_dir, img_name = os.path.split(original_image_path)\n",
    "    class_name = os.path.basename(os.path.dirname(original_image_path))\n",
    "    new_dir = os.path.join(dataset_dir, class_name)\n",
    "    os.makedirs(new_dir, exist_ok=True)\n",
    "    cropped_image_path = os.path.join(new_dir, img_name)\n",
    "    cv2.imwrite(cropped_image_path, cv2.cvtColor(cropped_image, cv2.COLOR_RGB2BGR))\n",
    "    return cropped_image_path\n",
    "\n",
    "def load_and_preprocess_image(base_dir, image_path, max_image_size):\n",
    "    image_path = os.path.join(base_dir, image_path)\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Cannot load image from {image_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    if max_image_size is not None:\n",
    "        img = cv2.resize(img, max_image_size)\n",
    "    \n",
    "    original_img = img.copy()\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img, original_img\n",
    "\n",
    "def detect_car(image, detector, score_threshold=0.5):\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    input_tensor = tf.expand_dims(input_tensor, axis=0)\n",
    "    detections = detector.signatures['serving_default'](input_tensor)\n",
    "\n",
    "    boxes = detections['detection_boxes'].numpy()[0]\n",
    "    class_ids = detections['detection_classes'].numpy()[0].astype(np.int64)\n",
    "    scores = detections['detection_scores'].numpy()[0]\n",
    "\n",
    "    return {\"boxes\": boxes, \"class_ids\": class_ids, \"scores\": scores}\n",
    "\n",
    "def process_chunk(data_chunk, base_dir, dataset_dir, detector, class_index, score_threshold, save_images, none_list_file, max_image_size):\n",
    "    batch_results = []\n",
    "    for idx, row in tqdm(data_chunk.iterrows(), total=data_chunk.shape[0]):\n",
    "        image_path = row['ImageFullPath']\n",
    "        image_np, original_image = load_and_preprocess_image(base_dir, image_path, max_image_size)\n",
    "\n",
    "        if image_np is None:\n",
    "            batch_results.append((image_path, None, None, None))\n",
    "            continue\n",
    "\n",
    "        detection_results = detect_car(image_np, detector, score_threshold)\n",
    "\n",
    "        boxes = detection_results[\"boxes\"]\n",
    "        class_ids = detection_results[\"class_ids\"]\n",
    "        scores = detection_results[\"scores\"]\n",
    "\n",
    "        img_h, img_w, _ = original_image.shape\n",
    "\n",
    "        max_area = 0\n",
    "        prioritized_box = None\n",
    "        best_score = 0\n",
    "\n",
    "        for box, score, label in zip(boxes, scores, class_ids):\n",
    "            if score < score_threshold or label != class_index:\n",
    "                continue\n",
    "            ymin, xmin, ymax, xmax = box\n",
    "            area = (xmax - xmin) * (ymax - ymin)\n",
    "\n",
    "            if area > max_area:\n",
    "                max_area = area\n",
    "                prioritized_box = (xmin, ymin, xmax, ymax)\n",
    "                best_score = score\n",
    "\n",
    "        cropped_image_path = None\n",
    "        prioritized_box_pixels = None\n",
    "\n",
    "        if prioritized_box is not None:\n",
    "            xmin, ymin, xmax, ymax = prioritized_box\n",
    "            left = int(xmin * img_w)\n",
    "            right = int(xmax * img_w)\n",
    "            top = int(ymin * img_h)\n",
    "            bottom = int(ymax * img_h)\n",
    "\n",
    "            cropped_image = original_image[top:bottom, left:right]\n",
    "            if save_images:\n",
    "                cropped_image_path = save_cropped_image(cropped_image, image_path, dataset_dir)\n",
    "            prioritized_box_pixels = (left, right, top, bottom)\n",
    "        else:\n",
    "            if save_images:\n",
    "                cropped_image = original_image \n",
    "                cropped_image_path = save_cropped_image(cropped_image, image_path, dataset_dir)\n",
    "            with open(os.path.join(dataset_dir, none_list_file), 'a') as f:\n",
    "                f.write(f\"{image_path}, No_Car_Detected\\n\")\n",
    "\n",
    "        batch_results.append((image_path, cropped_image_path, prioritized_box_pixels, best_score))\n",
    "\n",
    "    return batch_results\n",
    "\n",
    "def process_car_detection(base_dir='./',\n",
    "                          dataset_dir='./',\n",
    "                          data=None,\n",
    "                          file_name='CarDataset.csv',\n",
    "                          detector=None,\n",
    "                          partition=False,\n",
    "                          partition_size=1000,\n",
    "                          batch_size=10,\n",
    "                          chunk_size=100, \n",
    "                          class_index=3,\n",
    "                          score_threshold=0.5,\n",
    "                          save_images=True,\n",
    "                          none_list_file='NoneList.csv',\n",
    "                          max_image_size=(640, 640)\n",
    "                          ):\n",
    "    data_path = os.path.join(dataset_dir, file_name)\n",
    "    results_list = []\n",
    "\n",
    "    total_rows = sum(1 for row in open(data_path)) -1\n",
    "    num_chunks = math.ceil(total_rows / chunk_size)\n",
    "\n",
    "    print(f\"Total number of chunks: {num_chunks}\")\n",
    "\n",
    "    chunk_num = 0 \n",
    "    for chunk in pd.read_csv(data_path, chunksize=chunk_size):\n",
    "        chunk_num += 1\n",
    "        print(f\"Processing chunk {chunk_num} out of {num_chunks} (Size: {len(chunk)} rows)\")\n",
    "\n",
    "        if partition:\n",
    "            chunk = chunk.sample(n=min(partition_size, len(chunk)), random_state=42).reset_index(drop=True)\n",
    "            print(f\"  - Processing {len(chunk)} images after partitioning...\")\n",
    "\n",
    "        total_batches = math.ceil(len(chunk) / batch_size)\n",
    "        print(f\"  - Total batches in this chunk: {total_batches}\")\n",
    "\n",
    "        for i in range(0, len(chunk), batch_size):\n",
    "            batch_data = chunk[i:i + batch_size]\n",
    "            batch_results = process_chunk(batch_data, base_dir, dataset_dir, detector, class_index, score_threshold, save_images, none_list_file, max_image_size)\n",
    "            results_list.extend(batch_results)\n",
    "\n",
    "            del batch_data\n",
    "            del batch_results\n",
    "            gc.collect()\n",
    "\n",
    "    return results_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = process_car_detection(base_dir=base_dir,\n",
    "                                     dataset_dir=dataset_dir,\n",
    "                                     data=df,\n",
    "                                     detector=detector,\n",
    "                                     batch_size = 128,\n",
    "                                     chunk_size = 1024,\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(results_list,\n",
    "                      data=None,\n",
    "                      base_dir='./',\n",
    "                      dataset_dir='./',\n",
    "                      file_name='CarDataset.csv',\n",
    "                      partition=False,\n",
    "                      partition_size=10,\n",
    "                      ):\n",
    "    \n",
    "    if data is None:\n",
    "        data_path = os.path.join(dataset_dir, file_name)\n",
    "        data = pd.read_csv(data_path)\n",
    "    if partition:\n",
    "        start_index = (partition - 1) * partition_size\n",
    "        end_index = min(start_index + partition_size, len(results_list))\n",
    "        results_list = results_list[start_index:end_index]\n",
    "\n",
    "    for i, (image_path, cropped_image_path, bbox, score) in tqdm(enumerate(results_list), total=len(results_list), desc=\"Processing images: \"):\n",
    "        image_path = os.path.join(base_dir, image_path)\n",
    "        try:\n",
    "            original_image = cv2.imread(image_path)\n",
    "            original_image = cv2.resize(original_image, (640, 640))\n",
    "            original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "            if original_image is None:\n",
    "                print(f\"Warning: Could not load image at {image_path}. Skipping.\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        ax[0].grid(False)\n",
    "        ax[1].grid(False)\n",
    "\n",
    "        ax[0].imshow(original_image)\n",
    "        ax[0].set_title(f\"Original Image: {os.path.basename(image_path)}\")\n",
    "\n",
    "        if bbox:\n",
    "            left, right, top, bottom = bbox\n",
    "            rect = patches.Rectangle((left, top), right - left, bottom - top,\n",
    "                                     linewidth=2, edgecolor='r', facecolor='none')\n",
    "            ax[0].add_patch(rect)\n",
    "            ax[0].text(left, top - 5, f\"Car (Score: {score:.2f})\", color='red', fontsize=10)\n",
    "\n",
    "        if cropped_image_path is not None:\n",
    "            cropped_image = cv2.imread(cropped_image_path)\n",
    "            ax[1].imshow(cropped_image)\n",
    "            ax[1].set_title(\"Cropped Car\")\n",
    "        else:\n",
    "            ax[1].set_title(\"No Car Detected Above Threshold\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(base_dir=base_dir,\n",
    "                  dataset_dir = dataset_dir,\n",
    "                    results_list = results_list,\n",
    "                    data=df,\n",
    "                    partition=True,\n",
    "                    partition_size=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
