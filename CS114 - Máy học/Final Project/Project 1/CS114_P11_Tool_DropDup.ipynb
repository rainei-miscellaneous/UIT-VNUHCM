{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SỬ DỤNG ĐẶC TRƯNG ĐÃ ĐƯỢC TRÍCH XUẤT ĐỂ LOẠI BỎ ẢNH TRÙNG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Thông tin của tác giả, ngày cập nhật**\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Thành viên nhóm**:\n",
    "- **Trần Đình Khánh Đăng - 22520195**\n",
    "- **Tăng Nhất - 22521027**\n",
    "- **Lê Minh Nhựt - 22521060**\n",
    "\n",
    "**Ngày cập nhật**: 22/01/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import patches\n",
    "from scipy.spatial.distance import cdist, euclidean\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.applications import (MobileNet, MobileNetV2, MobileNetV3Small, MobileNetV3Large, \n",
    "                                           ResNet50, ResNet101, ResNet152,\n",
    "                                           VGG16, VGG19,\n",
    "                                           EfficientNetB0, EfficientNetB1, EfficientNetB7,\n",
    "                                           InceptionV3, Xception)\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Khởi tạo đường dẫn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '/kaggle/working/dataset'\n",
    "extracted_features_file_name='extracted_features.npz'\n",
    "\n",
    "cropped_file_name = 'cropped_dataset.csv'\n",
    "cropped_dropdup_file_name = 'dropdup_dataset.csv'\n",
    "\n",
    "cropped_base_dir= '/kaggle/input/cs114-cropped-full-dataset/dataset'\n",
    "cropped_dataset_name = 'cropped_CarDataset.csv'\n",
    "cropped_file_name_cars = 'cropped_CarDataset-1.csv'\n",
    "cropped_file_name_categories = 'cropped_CarDataset-2.csv'\n",
    "cropped_extracted_features_file_name='cropped_extracted_features.npz'\n",
    "\n",
    "cropped_dropdup_extracted_features_file_name='dropdup_extracted_features.npz'\n",
    "cropped_dropdup_extracted_features_csv = 'dropdup_extracted_features.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_one_img(image_path, model, input_shape=(224, 224)):\n",
    "    img = image.load_img(image_path, target_size=input_shape)\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = model.predict(x, verbose=0)\n",
    "    return features.flatten()\n",
    "\n",
    "def extract_features(data=None,\n",
    "                     base_dir='./',\n",
    "                     dataset_dir='./',\n",
    "                     file_csv='CarDataset-Splits-1-Train.csv',\n",
    "                     model_name='MobileNet',\n",
    "                     input_shape=(224, 224),\n",
    "                     partition=False,\n",
    "                     partition_size=1000,\n",
    "                     random_state=42,\n",
    "                     save_result=False,\n",
    "                     save_name='extracted_features-Splits-1.npz'):\n",
    "    models = {\n",
    "        'MobileNet': MobileNet,\n",
    "        'MobileNetV2': MobileNetV2,\n",
    "        'MobileNetV3Small': MobileNetV3Small,\n",
    "        'MobileNetV3Large': MobileNetV3Large,\n",
    "        'ResNet50': ResNet50,\n",
    "        'ResNet101': ResNet101,\n",
    "        'ResNet152': ResNet152,\n",
    "        'VGG16': VGG16,\n",
    "        'VGG19': VGG19,\n",
    "        'EfficientNetB0': EfficientNetB0,\n",
    "        'EfficientNetB1': EfficientNetB1,\n",
    "        'EfficientNetB7': EfficientNetB7,\n",
    "        'InceptionV3': InceptionV3,\n",
    "        'Xception': Xception\n",
    "    }\n",
    "\n",
    "    if model_name not in models:\n",
    "        model_name = 'MobileNet'\n",
    "\n",
    "    device = '/device:GPU:0' if tf.config.list_physical_devices('GPU') else '/device:CPU:0'\n",
    "    with tf.device(device):\n",
    "        model = models[model_name](weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "    if data is None:\n",
    "        data = pd.read_csv(os.path.join(dataset_dir, file_csv))\n",
    "\n",
    "    if partition:\n",
    "        sampled_data = data.sample(n=min(partition_size, len(data)), random_state=random_state).reset_index(drop=True)\n",
    "        print(f\"Processing {len(sampled_data)} images out of {len(data)} available.\")\n",
    "    else:\n",
    "        sampled_data = data\n",
    "    print(\"Extracting features...\")\n",
    "\n",
    "    result = []\n",
    "    for _, row in tqdm(sampled_data.iterrows(), desc=\"Extracting Features\", total=len(sampled_data), file=sys.stdout, leave=True):\n",
    "        image_path = row[\"ImageFullPath\"]\n",
    "        categoryid = row[\"CategoryID\"]\n",
    "\n",
    "        full_path = os.path.join(base_dir, image_path)\n",
    "        try:\n",
    "            extracted_features = extract_feature_one_img(full_path, model, input_shape=input_shape)\n",
    "            result.append({'ImageFullPath': image_path, 'CategoryID': categoryid, 'Extracted Features': extracted_features})\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {full_path}: {e}. Skipping...\")\n",
    "\n",
    "    print(f\"Successfully processed {len(result)} images\")\n",
    "\n",
    "    if save_result:\n",
    "        save_path = os.path.join(dataset_dir, save_name)\n",
    "        np.savez(save_path, extracted_features=result)\n",
    "        print(f\"Extracted features saved to {save_path}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def load_features(file_path):\n",
    "    try:\n",
    "        # Tải file .npz và lấy dữ liệu 'extracted_features'\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        extracted_features = data['extracted_features']\n",
    "\n",
    "        # Đảm bảo định dạng giống như khi lưu\n",
    "        formatted_features = []\n",
    "        for item in extracted_features:\n",
    "            # Chuyển từng phần tử từ ndarray về dictionary với đúng format\n",
    "            formatted_features.append({\n",
    "                'ImageFullPath': item['ImageFullPath'],\n",
    "                'CategoryID': item['CategoryID'],\n",
    "                'Extracted Features': item['Extracted Features']\n",
    "            })\n",
    "\n",
    "        print(f\"Loaded extracted features from {file_path}\")\n",
    "        return formatted_features\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading features from {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cropped = pd.read_csv(cropped_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "extracted_features = extract_features(base_dir=cropped_base_dir,\n",
    "                                      dataset_dir=dataset_dir,\n",
    "                                      file_csv=cropped_file_name,\n",
    "                                      model_name='XceptionNet',\n",
    "                                      input_shape=(224, 224),\n",
    "                                      partition=False,\n",
    "                                      partition_size=1000,\n",
    "                                      save_result=True,\n",
    "                                      save_name=extracted_features_file_name\n",
    "                                      )\n",
    "print(f\"Total time: {(time.time()-st)/3600:.2f}h\")\n",
    "\n",
    "# Load extracted_features đã có từ trước\n",
    "# extracted_features = load_features('/kaggle/input/cs114-cropped-full-dataset/dataset/cropped_extracted_features.npz')\n",
    "# df_extracted_features = pd.DataFrame([feature for feature in tqdm(extracted_features, desc=\"Turning to DataFrame\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def to_grayscale(features_list):\n",
    "    processed_features = []\n",
    "    for feature in features_list:\n",
    "        if feature.ndim == 3:\n",
    "            gray_feature = rgb2gray(feature)\n",
    "        else:\n",
    "            gray_feature = feature\n",
    "        processed_features.append(gray_feature)\n",
    "    return np.array(processed_features)\n",
    "\n",
    "def find_duplicate(extracted_features=None,\n",
    "                   base_dir='./',  # Used for 'ssim'\n",
    "                   dataset_dir='./',\n",
    "                   extracted_file='extracted_features.npz',\n",
    "                   similarity_threshold=0.9,\n",
    "                   metric='cosine',\n",
    "                   save_result=False,\n",
    "                   save_name='duplicate_images.csv'):\n",
    "\n",
    "    if extracted_features is None:\n",
    "        file_path = os.path.join(dataset_dir, extracted_file)\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        extracted_features = data['extracted_features']\n",
    "\n",
    "    features_list = [x['Extracted Features'] for x in extracted_features]\n",
    "    image_path_list = [x['ImageFullPath'] for x in extracted_features]\n",
    "    categories_list = [x['CategoryID'] for x in extracted_features]\n",
    "    features_list = to_grayscale(features_list)\n",
    "\n",
    "    features_tensor = torch.tensor(np.array(features_list), device=device)\n",
    "\n",
    "    if metric == 'phash':\n",
    "        precomputed_hashes = {img_path: phash(Image.open(os.path.join(base_dir, img_path)))\n",
    "                              for img_path in tqdm(image_path_list, desc='Computing hashes', unit='image')}\n",
    "\n",
    "    duplicate_images = []\n",
    "    duplicate_indices = []\n",
    "    num_images = len(features_list)\n",
    "\n",
    "    for i in tqdm(range(num_images), desc=\"Processing images\", unit=\"image\"):\n",
    "        # Get the current feature and move it to GPU\n",
    "        current_feature = features_tensor[i].unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Compute similarities in batches\n",
    "        if metric == 'cosine':\n",
    "            # Compute cosine similarity using PyTorch\n",
    "            similarities = torch.nn.functional.cosine_similarity(\n",
    "                current_feature, features_tensor[i + 1:], dim=1\n",
    "            )\n",
    "            # Find indices where similarity > threshold\n",
    "            duplicate_mask = similarities > similarity_threshold\n",
    "            duplicate_indices_batch = torch.nonzero(duplicate_mask).squeeze(1) + i + 1\n",
    "\n",
    "        elif metric == 'euclidean':\n",
    "            # Compute Euclidean distance using PyTorch\n",
    "            distances = torch.cdist(current_feature, features_tensor[i + 1:], p=2).squeeze(0)\n",
    "            # Find indices where distance < (1 - threshold)\n",
    "            duplicate_mask = distances < (1.0 - similarity_threshold)\n",
    "            duplicate_indices_batch = torch.nonzero(duplicate_mask).squeeze(1) + i + 1\n",
    "\n",
    "        elif metric == 'ssim':\n",
    "            # SSIM is not easily parallelizable, so we fall back to CPU\n",
    "            for j in range(i + 1, num_images):\n",
    "                similarity = ssim(features_list[i].reshape(-1), features_list[j].reshape(-1), data_range=1)\n",
    "                if similarity > similarity_threshold:\n",
    "                    print(f\"Found duplicate images: {image_path_list[i]} and {image_path_list[j]} with SSIM {similarity}\")\n",
    "                    duplicate_images.append((image_path_list[i], image_path_list[j], similarity))\n",
    "                    duplicate_indices.append((i, j))\n",
    "\n",
    "        elif metric == 'phash':\n",
    "            # pHash is computed on CPU\n",
    "            hash1 = precomputed_hashes[image_path_list[i]]\n",
    "            for j in range(i + 1, num_images):\n",
    "                hash2 = precomputed_hashes[image_path_list[j]]\n",
    "                similarity = 1 - (hash1 - hash2) / len(hash1.hash)\n",
    "                if similarity > similarity_threshold:\n",
    "                    duplicate_images.append((image_path_list[i], image_path_list[j], similarity))\n",
    "                    duplicate_indices.append((i, j))\n",
    "                    print(f\"Found duplicate images: {image_path_list[i]} and {image_path_list[j]} with SSIM {similarity}\")\n",
    "\n",
    "        # For cosine and Euclidean, process the batch results\n",
    "        if metric in ['cosine', 'euclidean']:\n",
    "            for j in duplicate_indices_batch.cpu().numpy():\n",
    "                if metric == 'cosine':\n",
    "                    similarity = similarities[j - i - 1].item()\n",
    "                    print(f\"Found duplicate images: {image_path_list[i]} and {image_path_list[j]} with cosine similarity {similarity}\")\n",
    "                elif metric == 'euclidean':\n",
    "                    distance = distances[j - i - 1].item()\n",
    "                    print(f\"Found duplicate images: {image_path_list[i]} and {image_path_list[j]} with Euclidean distance {distance}\")\n",
    "                duplicate_images.append((image_path_list[i], image_path_list[j], similarity if metric == 'cosine' else distance))\n",
    "                duplicate_indices.append((i, j))\n",
    "\n",
    "    if save_result:\n",
    "        save_path = os.path.join(dataset_dir, save_name)\n",
    "        if metric in ['cosine', 'ssim', 'phash']:\n",
    "            df = pd.DataFrame(duplicate_images, columns=[\"Image1\", \"Image2\", \"Score\"])\n",
    "        else:\n",
    "            df = pd.DataFrame(duplicate_images, columns=[\"Image1\", \"Image2\", \"Distance\"])\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Saved to {save_path}\")\n",
    "\n",
    "    return duplicate_images, duplicate_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate_images = find_duplicate(extracted_features=extracted_features,\n",
    "#                                   base_dir=base_dir,\n",
    "#                                   dataset_dir=dataset_dir,\n",
    "#                                   save_result = True,\n",
    "#                                   metric='cosine') # Xịn nhất nhưng lâu nhất\n",
    "# duplicate_images_2 = find_duplicate(extracted_features=extracted_features,\n",
    "#                                   base_dir=base_dir,\n",
    "#                                   dataset_dir=dataset_dir,\n",
    "#                                   save_result = True,\n",
    "#                                   metric='ssim') # Tàm tạm, không xịn\n",
    "# duplicate_images_3, duplicate_indices_3 = find_duplicate(extracted_features=extracted_features[:10000],\n",
    "#                                   base_dir=base_dir,\n",
    "#                                   dataset_dir=dataset_dir,\n",
    "#                                   save_result = True,\n",
    "#                                   metric='euclidean') # Phế, nhưng nhanh\n",
    "# duplicate_images_4 = find_duplicate(extracted_features=extracted_features,\n",
    "#                                   base_dir=base_dir,\n",
    "#                                   dataset_dir=dataset_dir,\n",
    "#                                   save_result = True,\n",
    "#                                   metric='phash') # Ổn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiến hành loại bỏ ảnh trùng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_duplicate_images = []\n",
    "all_duplicate_indices = []\n",
    "\n",
    "num_images = len(extracted_features)\n",
    "chunk_size = 36740\n",
    "\n",
    "for i in tqdm(range(0, num_images, chunk_size), desc='Processing Batches', unit='batch'):\n",
    "    start_idx = i\n",
    "    end_idx = min(i + chunk_size, num_images) \n",
    "    chunk = extracted_features[start_idx:end_idx]\n",
    "    \n",
    "    # Find duplicates in the current chunk\n",
    "    duplicate_images, duplicate_indices = find_duplicate(\n",
    "        extracted_features=chunk,\n",
    "        base_dir=cropped_base_dir,\n",
    "        dataset_dir=dataset_dir,\n",
    "        save_result=True,\n",
    "        save_name=f'duplicate_images_{start_idx + 1}_{end_idx}.csv',\n",
    "        metric='cosine',\n",
    "        similarity_threshold=0.95\n",
    "    )\n",
    "    \n",
    "    all_duplicate_images.extend(duplicate_images)\n",
    "    all_duplicate_indices.extend(duplicate_indices)\n",
    "    \n",
    "    print(f\"Processed images {start_idx + 1} to {end_idx}\")\n",
    "\n",
    "df_combined = pd.DataFrame(all_duplicate_images, columns=[\"Image1\", \"Image2\", \"Distance\"])\n",
    "\n",
    "df_combined['Index1'] = [idx[0] for idx in all_duplicate_indices]\n",
    "df_combined['Index2'] = [idx[1] for idx in all_duplicate_indices]\n",
    "\n",
    "# Save the combined results to a CSV file\n",
    "combined_save_path = os.path.join(dataset_dir, \"cosine_combined_duplicate_images.csv\")\n",
    "df_combined.to_csv(combined_save_path, index=False)\n",
    "print(f\"Combined duplicate image results saved to {combined_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_duplicate_images = []\n",
    "all_duplicate_indices = []\n",
    "\n",
    "num_images = len(extracted_features)\n",
    "chunk_size = 36740\n",
    "\n",
    "for i in tqdm(range(0, num_images, chunk_size), desc='Processing Batches', unit='batch'):\n",
    "    start_idx = i\n",
    "    end_idx = min(i + chunk_size, num_images) \n",
    "    chunk = extracted_features[start_idx:end_idx]\n",
    "    \n",
    "    # Find duplicates in the current chunk\n",
    "    duplicate_images, duplicate_indices = find_duplicate(\n",
    "        extracted_features=chunk,\n",
    "        base_dir=cropped_base_dir,\n",
    "        dataset_dir=dataset_dir,\n",
    "        save_result=True,\n",
    "        save_name=f'duplicate_images_{start_idx + 1}_{end_idx}.csv',\n",
    "        metric='euclidean',\n",
    "        similarity_threshold=0.95\n",
    "    )\n",
    "    \n",
    "    all_duplicate_images.extend(duplicate_images)\n",
    "    all_duplicate_indices.extend(duplicate_indices)\n",
    "    \n",
    "    print(f\"Processed images {start_idx + 1} to {end_idx}\")\n",
    "\n",
    "df_combined = pd.DataFrame(all_duplicate_images, columns=[\"Image1\", \"Image2\", \"Distance\"])\n",
    "\n",
    "df_combined['Index1'] = [idx[0] for idx in all_duplicate_indices]\n",
    "df_combined['Index2'] = [idx[1] for idx in all_duplicate_indices]\n",
    "\n",
    "# Save the combined results to a CSV file\n",
    "combined_save_path = os.path.join(dataset_dir, \"euclidean_combined_duplicate_images.csv\")\n",
    "df_combined.to_csv(combined_save_path, index=False)\n",
    "print(f\"Combined duplicate image results saved to {combined_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df_extracted_features,\n",
    "                      duplicate_indices,\n",
    "                      removal_strategy='keep_first',\n",
    "                      dataset_dir='./',\n",
    "                      save_results=False,\n",
    "                      extracted_features_name='extracted_features.npz',\n",
    "                      extracted_dataframe_name='dropdup_extracted_features.csv'):\n",
    "    indices_to_remove = set()\n",
    "    removed_image_paths = []  \n",
    "\n",
    "    for i, j in duplicate_indices:\n",
    "        if removal_strategy == 'keep_first':\n",
    "            indices_to_remove.add(j)\n",
    "            removed_image_paths.append(df_extracted_features['ImageFullPath'].iloc[j])  \n",
    "        elif removal_strategy == 'keep_second':\n",
    "            indices_to_remove.add(i)\n",
    "            removed_image_paths.append(df_extracted_features['ImageFullPath'].iloc[i]) \n",
    "        elif removal_strategy == 'keep_smaller':\n",
    "            if 'ImageFullPath' not in df_extracted_features.columns:\n",
    "                print(\"Warning: 'ImageFullPath' column not found. Defaulting to 'keep_first'.\")\n",
    "                indices_to_remove.add(j)\n",
    "                removed_image_paths.append(df_extracted_features['ImageFullPath'].iloc[j])\n",
    "            else:\n",
    "                path_i = os.path.join(dataset_dir, df_extracted_features['ImageFullPath'].iloc[i])\n",
    "                path_j = os.path.join(dataset_dir, df_extracted_features['ImageFullPath'].iloc[j])\n",
    "                try:\n",
    "                    size_i = os.path.getsize(path_i)\n",
    "                    size_j = os.path.getsize(path_j)\n",
    "\n",
    "                    if size_i <= size_j:\n",
    "                        indices_to_remove.add(j)\n",
    "                        removed_image_paths.append(df_extracted_features['ImageFullPath'].iloc[j])  # Add removed image path\n",
    "                    else:\n",
    "                        indices_to_remove.add(i)\n",
    "                        removed_image_paths.append(df_extracted_features['ImageFullPath'].iloc[i])  # Add removed image path\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"Warning: One or both files not found: {path_i}, {path_j}. Defaulting to 'keep_first'.\")\n",
    "                    indices_to_remove.add(j)\n",
    "                    removed_image_paths.append(df_extracted_features['ImageFullPath'].iloc[j])  # Add removed image path\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid removal_strategy: {removal_strategy}\")\n",
    "\n",
    "    indices_to_remove = sorted(\n",
    "        [df_extracted_features.index[idx] for idx in indices_to_remove if idx < len(df_extracted_features.index)],\n",
    "        reverse=True\n",
    "    )\n",
    "    new_df_extracted_features = df_extracted_features.drop(indices_to_remove)\n",
    "\n",
    "    extracted_features_list = []\n",
    "    # Duyet qua tung dong dataframe\n",
    "    for _, row in tqdm(new_df_extracted_features.iterrows(), desc=\"Extracting Features\", total=len(new_df_extracted_features), file=sys.stdout, leave=True):\n",
    "        image_path = row[\"ImageFullPath\"]\n",
    "        categoryid = row[\"CategoryID\"]\n",
    "        extracted_feature = row[\"Extracted Features\"]\n",
    "        # Lay duong dan full\n",
    "        full_path = os.path.join(dataset_dir, image_path)\n",
    "        try:\n",
    "            extracted_features_list.append({'ImageFullPath': image_path, 'CategoryID': categoryid, 'Extracted Features': extracted_feature})\n",
    "        except Exception as e:\n",
    "            print(f\"Error at image {full_path}: {e}. Skipping...\")\n",
    "\n",
    "    print(f\"Successfully processed {len(extracted_features_list)} images\")\n",
    "    new_df_extracted_features['Extracted Features'] = [entry['Extracted Features'] for entry in extracted_features_list]\n",
    "    new_extracted_features = new_df_extracted_features.reset_index().drop('index', axis=1)\n",
    "    if save_results:\n",
    "        df_file_path = os.path.join(dataset_dir, extracted_dataframe_name)\n",
    "        features_file_path = os.path.join(dataset_dir, extracted_features_name)\n",
    "\n",
    "        new_df_extracted_features.to_csv(df_file_path, index=False)\n",
    "        print(f\"Saved DataFrame to: {df_file_path}\")\n",
    "\n",
    "        np.savez(features_file_path, extracted_features=extracted_features_list)\n",
    "        print(f\"Saved extracted features to: {features_file_path}\")\n",
    "\n",
    "    return new_df_extracted_features, extracted_features_list, removed_image_paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features = load_features('/kaggle/input/cs114-extracted-features/fulldata_extracted_features.npz')\n",
    "df_extracted_features = pd.DataFrame([feature for feature in tqdm(extracted_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_extracted_features, extracted_features_list, indices_to_remove = remove_duplicates(\n",
    "    df_extracted_features=df_extracted_features,\n",
    "    duplicate_indices=all_duplicate_indices,\n",
    "    removal_strategy='keep_first', \n",
    "    dataset_dir=dataset_dir,\n",
    "    save_results=True,\n",
    "    extracted_features_name=cropped_dropdup_extracted_features_file_name,\n",
    "    extracted_dataframe_name=cropped_dropdup_extracted_features_csv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_duplicate_images(image_pairs, base_dir):\n",
    "    rows = len(image_pairs)\n",
    "    fig, axes = plt.subplots(rows, 2, figsize=(10, rows * 3))\n",
    "\n",
    "    if rows == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, (image_path1, image_path2, scores) in tqdm(enumerate(image_pairs), desc=\"Displaying images\", total=len(image_pairs), unit='image'):\n",
    "        full_path1 = os.path.join(base_dir, image_path1)\n",
    "        full_path2 = os.path.join(base_dir, image_path2)\n",
    "\n",
    "        img1 = Image.open(full_path1)\n",
    "        img2 = Image.open(full_path2)\n",
    "\n",
    "        axes[i][0].imshow(img1)\n",
    "        axes[i][0].axis('off')\n",
    "        axes[i][0].set_title(image_path1.split('/')[-1])\n",
    "\n",
    "        axes[i][1].imshow(img2)\n",
    "        axes[i][1].axis('off')\n",
    "        axes[i][1].set_title(image_path2.split('/')[-1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_duplicate_images(all_duplicate_images[:10], base_dir=cropped_base_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
