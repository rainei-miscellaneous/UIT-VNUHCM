{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HUẤN LUYỆN VÀ ĐÁNH GIÁ MÔ HÌNH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Thông tin của tác giả, ngày cập nhật**\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Thành viên nhóm**:\n",
    "- **Trần Đình Khánh Đăng - 22520195**\n",
    "- **Tăng Nhất - 22521027**\n",
    "- **Lê Minh Nhựt - 22521060**\n",
    "\n",
    "**Ngày cập nhật**: 22/01/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install và Import thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \\\n",
    "    --extra-index-url=https://pypi.nvidia.com \\\n",
    "    \"cudf-cu12==24.12.*\" \"dask-cudf-cu12==24.12.*\" \"cuml-cu12==24.12.*\" \\\n",
    "    \"cugraph-cu12==24.12.*\" \"nx-cugraph-cu12==24.12.*\" \"cuspatial-cu12==24.12.*\" \\\n",
    "    \"cuproj-cu12==24.12.*\" \"cuxfilter-cu12==24.12.*\" \"cucim-cu12==24.12.*\" \\\n",
    "    \"pylibraft-cu12==24.12.*\" \"raft-dask-cu12==24.12.*\" \"cuvs-cu12==24.12.*\" \\\n",
    "    \"nx-cugraph-cu12==24.12.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tqdm.notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cluster import (KMeans, DBSCAN, AgglomerativeClustering, \n",
    "                             MeanShift, Birch, SpectralClustering)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from cuml.svm import SVC as cuSVC\n",
    "from cuml.ensemble import RandomForestClassifier as cuRFClassifier\n",
    "from cuml.neighbors import KNeighborsClassifier as cuKNNClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Khởi tạo đường dẫn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '/kaggle/working/dataset'\n",
    "\n",
    "cropped_file_name = 'cropped_dataset.csv'\n",
    "cropped_dropdup_file_name = 'dropdup_dataset.csv'\n",
    "\n",
    "categories = ['Others', 'Honda', 'Hyundai', 'KIA', 'Mazda', 'Mitsubishi', 'Suzuki', 'Toyota', 'VinFast']\n",
    "\n",
    "base_dir = '/kaggle/input/cs114-final-project-full-dataset'\n",
    "dataset_name = 'CarDataset.csv'\n",
    "file_name_cars = 'CarDataset-1.csv'\n",
    "file_name_categories = 'CarDataset-2.csv'\n",
    "extracted_features_file_name='extracted_features.npz'\n",
    "\n",
    "cropped_base_dir= '/kaggle/input/cs114-cropped-full-dataset/dataset'\n",
    "cropped_dataset_name = 'cropped_CarDataset.csv'\n",
    "cropped_file_name_cars = 'cropped_CarDataset-1.csv'\n",
    "cropped_file_name_categories = 'cropped_CarDataset-2.csv'\n",
    "cropped_extracted_features_file_name='cropped_extracted_features.npz'\n",
    "\n",
    "cropped_dropdup_base_dir = '/kaggle/input/cs114-cropped-full-dataset/dataset'\n",
    "cropped_dropdup_dataset_name = 'cropped_dropdup_CarDataset.csv'\n",
    "cropped_dropdup_file_name_cars = 'cropped_dropdup_CarDataset-1.csv'\n",
    "cropped_dropdup_file_name_categories = 'cropped_dropdup_CarDataset-2.csv'\n",
    "cropped_dropdup_extracted_features_file_name='dropdup_extracted_features.npz'\n",
    "cropped_dropdup_extracted_features_csv = 'dropdup_extracted_features.csv'\n",
    "\n",
    "augmented_base_dir='/kaggle/input/cs114-augmented-dataset/augmented_images'\n",
    "augmented_dataset_name = 'augmented_CarDataset.csv'\n",
    "augmented_file_name_cars = 'augmented_CarDataset-1.csv'\n",
    "augmented_file_name_categories = 'augmented_CarDataset-2.csv'\n",
    "augmented_extracted_features_file_name='augmented_extracted_features.npz'\n",
    "\n",
    "full_augmented_base_dir='/kaggle/input/cs114-full-augmented-dataset/full_augmented_dataset'\n",
    "full_augmented_dataset_name = 'full_augmented_CarDataset.csv'\n",
    "full_augmented_file_name_cars = 'full_augmented_CarDataset-1.csv'\n",
    "full_augmented_file_name_categories = 'full_augmented_CarDataset-2.csv'\n",
    "full_augmented_extracted_features_file_name='full_augmented_extracted_features.npz'\n",
    "\n",
    "def get_indexing(categories):\n",
    "    indexing = {category: idx for idx, category in enumerate(categories)}\n",
    "    invert_indexing = {idx: category for category, idx in indexing.items()}\n",
    "    return indexing, invert_indexing\n",
    "\n",
    "indexing, invert_indexing = get_indexing(categories)\n",
    "\n",
    "num_splits = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Khai báo một số hàm cần thiết**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(base_dir='./',\n",
    "                dataset_dir='/',\n",
    "                categories=['Others', 'Honda', 'Hyundai', 'KIA', 'Mazda', 'Mitsubishi', 'Suzuki', 'Toyota', 'VinFast'],\n",
    "                save_csv=False,\n",
    "                file_name='CarDataset.csv',\n",
    "                ) -> pd.DataFrame:\n",
    "    \n",
    "    os.makedirs(dataset_dir, exist_ok=True)  # Tạo thư mục đầu ra nếu chưa tồn tại\n",
    "\n",
    "    path_list = []  # Lưu đường dẫn đầy đủ của hình ảnh\n",
    "    categoryid_list = []  # Lưu mã danh mục tương ứng với từng hình ảnh\n",
    "\n",
    "\n",
    "    student_ids_pattern = r'(\\d{8}(?:-\\d{8})*)' # Lấy MSSV hợp lệ (đủ 8 số)\n",
    "    categories_pattern = '|'.join(categories) # Lấy hiệu xe hợp lệ\n",
    "    file_extension_pattern = r'\\.\\d+\\.(jpg|jpeg|png)$' # Lấy extension hợp lệ (chỉ chấp nhận file .jpg, .jpeg và .png)\n",
    "    # Regex lấy tên file hợp lệ\n",
    "    accepted_filename = re.compile(fr'{student_ids_pattern}\\.({categories_pattern}){file_extension_pattern}')\n",
    "\n",
    "    for category in tqdm(categories, desc=\"Processing categories\"): # Duyệt qua các hiệu xe\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        if os.path.isdir(category_path): # Kiểm tra nếu thư mục tồn tại\n",
    "            for filename in os.listdir(category_path):\n",
    "                match = accepted_filename.match(filename)\n",
    "                if match: # Chỉ xử lý file có tên hợp lệ\n",
    "                    _, car_category, _ = match.groups()\n",
    "                    if car_category in categories:\n",
    "                        full_path = os.path.join(car_category, filename)\n",
    "                        path_list.append(full_path)\n",
    "                        categoryid_list.append(indexing[car_category])\n",
    "    # Tạo DataFrame từ danh sách đường dẫn và mã hiệu xe\n",
    "    df = pd.DataFrame({\n",
    "        'ImageFullPath': path_list,\n",
    "        'CategoryID': categoryid_list\n",
    "    })\n",
    "\n",
    "    # Lưu CSV nếu cần\n",
    "    if save_csv:\n",
    "        output_file = os.path.join(dataset_dir, file_name)\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"{file_name} saved to {output_file}\")\n",
    "\n",
    "    return df\n",
    "def augmented_get_dataset(base_dir='./',\n",
    "                dataset_dir='/',\n",
    "                categories=['Others', 'Honda', 'Hyundai', 'KIA', 'Mazda', 'Mitsubishi', 'Suzuki', 'Toyota', 'VinFast'],\n",
    "                save_csv=False,\n",
    "                file_name='augmented_CarDataset.csv',\n",
    "                ) -> pd.DataFrame:\n",
    "\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    path_list = []\n",
    "    categoryid_list = []\n",
    "\n",
    "    student_ids_pattern = r'(\\d{8}(?:-\\d{8})*)'\n",
    "    categories_pattern = '|'.join(categories)\n",
    "    file_extension_pattern = r'\\.(jpg|jpeg|png)$'\n",
    "    \n",
    "    # Updated regex to handle files like \"20520918.Mitsubishi.10_augmented_1.jpg\"\n",
    "    accepted_filename = re.compile(fr'{student_ids_pattern}\\.({categories_pattern})\\.[\\w-]+{file_extension_pattern}')\n",
    "\n",
    "    for category in tqdm(categories, desc=\"Processing categories\"):\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        if os.path.isdir(category_path):\n",
    "            for filename in os.listdir(category_path):\n",
    "                match = accepted_filename.match(filename)\n",
    "                if match:\n",
    "                    _, car_category, _ = match.groups()\n",
    "                    if car_category in categories:\n",
    "                        full_path = os.path.join(category, filename)  # Relative path within base_dir\n",
    "                        path_list.append(full_path)\n",
    "                        categoryid_list.append(indexing[car_category])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'ImageFullPath': path_list,\n",
    "        'CategoryID': categoryid_list\n",
    "    })\n",
    "\n",
    "    if save_csv:\n",
    "        output_file = os.path.join(dataset_dir, file_name)\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"{file_name} saved to {output_file}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_kfold_datasets(data=None,\n",
    "                        num_splits=5,\n",
    "                        base_dir='./',\n",
    "                        dataset_dir='./',\n",
    "                        save_csv=False,\n",
    "                        file_name='CarDataset.csv',\n",
    "                        prefix=None,\n",
    "                        random_state=42,\n",
    "                        ):\n",
    "    os.makedirs(dataset_dir, exist_ok=True)  # Tạo thư mục đầu ra nếu chưa tồn tại\n",
    "    if data == None:\n",
    "      # Đọc dữ liệu từ file CSV ban đầu\n",
    "      data_path = os.path.join(dataset_dir, file_name)\n",
    "      data = pd.read_csv(data_path)\n",
    "\n",
    "    path_list = data['ImageFullPath'].values # Danh sách đường dẫn hình ảnh\n",
    "    categoryid_list = data['CategoryID'].values # Danh sách mã hiệu xe tương ứng\n",
    "\n",
    "    train_splits = []  # Danh sách chứa các DataFrame tập train\n",
    "    test_splits = []  # Danh sách chứa các DataFrame tập test\n",
    "\n",
    "    # Khởi tạo đối tượng KFold\n",
    "    kfold = KFold(n_splits=num_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(path_list)):\n",
    "        # Đường dẫn file train/test cho từng tập con\n",
    "        if prefix is not None:\n",
    "            train_file_path = os.path.join(dataset_dir, f'{prefix}_CarDataset-Splits-{i + 1}-Train.csv')\n",
    "            test_file_path = os.path.join(dataset_dir, f'{prefix}_CarDataset-Splits-{i + 1}-Test.csv')\n",
    "        else:\n",
    "            train_file_path = os.path.join(dataset_dir, f'CarDataset-Splits-{i + 1}-Train.csv')\n",
    "            test_file_path = os.path.join(dataset_dir, f'CarDataset-Splits-{i + 1}-Test.csv')\n",
    "        # Tạo DataFrame cho tập train và test\n",
    "        train_data = pd.DataFrame({\n",
    "            'ImageFullPath': [path_list[idx] for idx in train_index],\n",
    "            'CategoryID': [categoryid_list[idx] for idx in train_index]\n",
    "        })\n",
    "        test_data = pd.DataFrame({\n",
    "            'ImageFullPath': [path_list[idx] for idx in test_index],\n",
    "            'CategoryID': [categoryid_list[idx] for idx in test_index]\n",
    "        })\n",
    "\n",
    "        if save_csv:\n",
    "            # Lưu DataFrame ra file CSV\n",
    "            train_data.to_csv(train_file_path, index=False)\n",
    "            test_data.to_csv(test_file_path, index=False)\n",
    "\n",
    "            print(f\"Train fold {i + 1} saved to {train_file_path}\")\n",
    "            print(f\"Test fold {i + 1} saved to {test_file_path}\")\n",
    "\n",
    "        train_splits.append(train_data)\n",
    "        test_splits.append(test_data)\n",
    "\n",
    "    return train_splits, test_splits\n",
    "def display_splits(train_splits, test_splits, max_files=5):\n",
    "    for i, (train_data, test_data) in enumerate(zip(train_splits, test_splits)):\n",
    "        print(f\"\\n=================== Split {i + 1} ===================\")\n",
    "\n",
    "        print(\"Train Files:\")\n",
    "        train_files = train_data['ImageFullPath'].tolist()\n",
    "        for file in train_files[:max_files]:\n",
    "            print(file)\n",
    "        if len(train_files) > max_files:\n",
    "            print(f\"... and {len(train_files) - max_files} more\")\n",
    "\n",
    "        print(\"\\nTest Files:\")\n",
    "        test_files = test_data['ImageFullPath'].tolist()\n",
    "        for file in test_files[:max_files]:\n",
    "            print(file)\n",
    "        if len(test_files) > max_files:\n",
    "            print(f\"... and {len(test_files) - max_files} more\")\n",
    "\n",
    "def display_images(csv_file='CarDataset-Splits-1-Train.csv',\n",
    "                   base_dir='./',\n",
    "                   dataset_dir='./',\n",
    "                   num_imgs_per_row=10,\n",
    "                   img_height=150,\n",
    "                   img_width=150,):\n",
    "    # Đọc file CSV\n",
    "    csv_path = os.path.join(dataset_dir, csv_file)\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Lấy danh sách các CategoryID\n",
    "    categories = df['CategoryID'].unique()\n",
    "\n",
    "    # Thiết lập vùng visualize/Điều chỉnh figsize cho phù hợp với số lượng ảnh\n",
    "    fig_height = len(categories) * (img_height / 100)\n",
    "    fig_width = num_imgs_per_row * (img_width / 100)\n",
    "    plt.figure(figsize=(fig_height, fig_width))\n",
    "\n",
    "    for i, category in enumerate(tqdm(categories, desc=\"Displaying images\")):\n",
    "        # Lấy ảnh thuộc category hiện tại\n",
    "        category_imgs = df[df['CategoryID'] == category]['ImageFullPath'].tolist()\n",
    "\n",
    "        # Chọn ngẫu nhiên ảnh\n",
    "        selected_imgs = random.sample(category_imgs, min(len(category_imgs), num_imgs_per_row))\n",
    "\n",
    "        # Tạo subplot cho CategoryID (đặt nó ở cột đầu tiên mỗi hàng)\n",
    "        ax = plt.subplot(len(categories), num_imgs_per_row + 1, i * (num_imgs_per_row + 1) + 1)\n",
    "        ax.text(0.5, 0.5, invert_indexing[category],\n",
    "                ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # Hiển thị các ảnh trong hàng\n",
    "        for j, img_path in enumerate(selected_imgs):\n",
    "            ax = plt.subplot(len(categories), num_imgs_per_row + 1, i * (num_imgs_per_row + 1) + j + 2)\n",
    "            try:\n",
    "                img = Image.open(os.path.join(base_dir, img_path))\n",
    "                img = img.resize((img_width, img_height))\n",
    "                ax.imshow(img)\n",
    "                ax.axis(\"off\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "    # Điều chỉnh layout\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def plot_class_distributions(splits, mode='train'):\n",
    "    num_splits = len(splits)\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_splits, figsize=(5 * num_splits, 5), sharey=True)\n",
    "    title = f\"Class Distributions Across {'Training' if mode == 'train' else 'Testing'} Splits\"\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        class_counts = splits[i]['CategoryID'].value_counts().sort_index()\n",
    "        class_counts.plot(kind='bar', ax=ax)\n",
    "        ax.set_title(f'Car-Splits-{i + 1}-Train' if mode == 'train' else f'Car-Splits-{i + 1}-Test')\n",
    "        ax.set_xlabel('CategoryID')\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Số lượng ảnh')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(file_path):\n",
    "    try:\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        extracted_features = data['extracted_features']\n",
    "\n",
    "        formatted_features = []\n",
    "        for item in extracted_features:\n",
    "            formatted_features.append({\n",
    "                'ImageFullPath': item['ImageFullPath'],\n",
    "                'CategoryID': item['CategoryID'],\n",
    "                'Extracted Features': item['Extracted Features']\n",
    "            })\n",
    "\n",
    "        print(f\"Loaded extracted features from {file_path}\")\n",
    "        return formatted_features\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading features from {file_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "def load_split_with_features(csv_file,\n",
    "                             features_df):\n",
    "    split_df = pd.read_csv(csv_file)\n",
    "    merged_df = pd.merge(split_df, features_df, on='ImageFullPath')\n",
    "    merged_df = merged_df[['ImageFullPath', 'CategoryID', 'Extracted Features']]\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load các đặc trưng đã được trích xuất**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features = load_features('/kaggle/input/cs114-extracted-features/fulldata_extracted_features.npz')\n",
    "df_extracted_features = pd.DataFrame([feature for feature in tqdm(extracted_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_extracted_features = load_features('/kaggle/input/cs114-cropped-full-dataset/dataset/cropped_extracted_features.npz')\n",
    "df_cropped_extracted_features = pd.DataFrame([feature for feature in tqdm(cropped_extracted_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_dropdup_extracted_features = load_features('/kaggle/input/cs114-extracted-features/dropdup_extracted_features.npz')\n",
    "df_cropped_dropdup_extracted_features = pd.DataFrame([feature for feature in tqdm(cropped_dropdup_extracted_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_extracted_features = load_features('/kaggle/input/cs114-augmented-dataset/augmented_images/augmented_extracted_features.npz')\n",
    "df_augmented_extracted_features = pd.DataFrame([feature for feature in tqdm(augmented_extracted_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_augmented_extracted_features = load_features('/kaggle/input/cs114-full-augmented-dataset/full_augmented_extracted_features.npz')\n",
    "df_full_augmented_extracted_features = pd.DataFrame([feature for feature in tqdm(full_augmented_extracted_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_get_dataset(base_dir='./',\n",
    "                dataset_dir='/',\n",
    "                categories=['Others', 'Honda', 'Hyundai', 'KIA', 'Mazda', 'Mitsubishi', 'Suzuki', 'Toyota', 'VinFast'],\n",
    "                save_csv=False,\n",
    "                file_name='augmented_CarDataset.csv',\n",
    "                ) -> pd.DataFrame:\n",
    "\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    path_list = []\n",
    "    categoryid_list = []\n",
    "\n",
    "    student_ids_pattern = r'(\\d{8}(?:-\\d{8})*)'\n",
    "    categories_pattern = '|'.join(categories)\n",
    "    file_extension_pattern = r'\\.(jpg|jpeg|png)$'\n",
    "    \n",
    "    # Updated regex to handle files like \"20520918.Mitsubishi.10_augmented_1.jpg\"\n",
    "    accepted_filename = re.compile(fr'{student_ids_pattern}\\.({categories_pattern})\\.[\\w-]+{file_extension_pattern}')\n",
    "\n",
    "    for category in tqdm(categories, desc=\"Processing categories\"):\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        if os.path.isdir(category_path):\n",
    "            for filename in os.listdir(category_path):\n",
    "                match = accepted_filename.match(filename)\n",
    "                if match:\n",
    "                    _, car_category, _ = match.groups()\n",
    "                    if car_category in categories:\n",
    "                        full_path = os.path.join(category, filename)  # Relative path within base_dir\n",
    "                        path_list.append(full_path)\n",
    "                        categoryid_list.append(indexing[car_category])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'ImageFullPath': path_list,\n",
    "        'CategoryID': categoryid_list\n",
    "    })\n",
    "\n",
    "    if save_csv:\n",
    "        output_file = os.path.join(dataset_dir, file_name)\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"{file_name} saved to {output_file}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tạo các Dataframe để tiến hành Split dữ liệu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_dataset(base_dir=cropped_base_dir,\n",
    "                            dataset_dir=dataset_dir,\n",
    "                            file_name=dataset_name,\n",
    "                            save_csv=True)\n",
    "cropped_data = get_dataset(base_dir=cropped_base_dir,\n",
    "                            dataset_dir=dataset_dir,\n",
    "                            file_name=cropped_dataset_name,\n",
    "                            save_csv=True)\n",
    "cropped_dropdup_data = get_dataset(base_dir=cropped_base_dir,\n",
    "                            dataset_dir=dataset_dir,\n",
    "                            file_name=cropped_dropdup_dataset_name,\n",
    "                            save_csv=True)\n",
    "augmented_data = augmented_get_dataset(base_dir=augmented_base_dir,\n",
    "                            dataset_dir=dataset_dir,\n",
    "                            file_name=augmented_dataset_name,\n",
    "                            save_csv=True)\n",
    "full_augmented_data = augmented_get_dataset(base_dir=full_augmented_base_dir,\n",
    "                            dataset_dir=dataset_dir,\n",
    "                            file_name=full_augmented_dataset_name,\n",
    "                            save_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_splits, test_splits = split_kfold_datasets(num_splits=num_splits,\n",
    "                                                 dataset_dir=dataset_dir,\n",
    "                                                 file_name=dataset_name,\n",
    "                                                 save_csv=True,\n",
    "                                                 random_state=42)\n",
    "cropped_train_splits, cropped_test_splits = split_kfold_datasets(num_splits=num_splits,\n",
    "                                                 dataset_dir=dataset_dir,\n",
    "                                                 file_name=cropped_dataset_name,\n",
    "                                                 save_csv=True,\n",
    "                                                 prefix='cropped',\n",
    "                                                 random_state=42)\n",
    "cropped_dropdup_train_splits, cropped_dropdup_test_splits = split_kfold_datasets(num_splits=num_splits,\n",
    "                                                 dataset_dir=dataset_dir,\n",
    "                                                 file_name=cropped_dropdup_dataset_name,\n",
    "                                                 save_csv=True,\n",
    "                                                 prefix='cropped_dropdup',                                                                              \n",
    "                                                 random_state=42)\n",
    "augmented_train_splits, augmented_test_splits = split_kfold_datasets(num_splits=num_splits,\n",
    "                                                 dataset_dir=dataset_dir,\n",
    "                                                 file_name=augmented_dataset_name,\n",
    "                                                 prefix='augmented',\n",
    "                                                 save_csv=True,\n",
    "                                                 random_state=42)\n",
    "full_augmented_train_splits, full_augmented_test_splits = split_kfold_datasets(num_splits=num_splits,\n",
    "                                                 dataset_dir=dataset_dir,\n",
    "                                                 file_name=full_augmented_dataset_name,\n",
    "                                                 prefix='full_augmented',\n",
    "                                                 save_csv=True,\n",
    "                                                 random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_splits(train_splits, test_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distributions(train_splits, mode='train')\n",
    "plot_class_distributions(test_splits, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_splits(cropped_train_splits, cropped_test_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distributions(cropped_train_splits, mode='train')\n",
    "plot_class_distributions(cropped_test_splits, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_splits(cropped_dropdup_train_splits, cropped_dropdup_test_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distributions(cropped_dropdup_train_splits, mode='train')\n",
    "plot_class_distributions(cropped_dropdup_test_splits, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_splits(augmented_train_splits, augmented_test_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distributions(augmented_train_splits, mode='train')\n",
    "plot_class_distributions(augmented_test_splits, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_splits(full_augmented_train_splits, full_augmented_test_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distributions(full_augmented_train_splits, mode='train')\n",
    "plot_class_distributions(full_augmented_test_splits, mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Đưa dữ liệu từ các splits vào các biến**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(train_files, test_files, features_df):\n",
    "    train_splits = []\n",
    "    test_splits = []\n",
    "\n",
    "    def load_split_with_features(csv_file, features_df):\n",
    "        split_df = pd.read_csv(csv_file)\n",
    "        merged_df = pd.merge(split_df, features_df, on='ImageFullPath')\n",
    "        merged_df = merged_df[['ImageFullPath', 'CategoryID', 'Extracted Features']]\n",
    "        return merged_df\n",
    "\n",
    "    for train_file in train_files:\n",
    "      train_split_df = load_split_with_features(train_file, features_df)\n",
    "      train_splits.append(train_split_df)\n",
    "\n",
    "    for test_file in test_files:\n",
    "      test_split_df = load_split_with_features(test_file, features_df)\n",
    "      test_splits.append(test_split_df)\n",
    "\n",
    "    return train_splits, test_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_extracted_features.drop('CategoryID', axis=1)\n",
    "df_cropped_features = df_cropped_extracted_features.drop('CategoryID', axis=1)\n",
    "df_cropped_dropdup_features = df_cropped_dropdup_extracted_features.drop('CategoryID', axis=1)\n",
    "df_augmented_features = df_augmented_extracted_features.drop('CategoryID', axis=1)\n",
    "df_full_augmented_features = df_full_augmented_extracted_features.drop('CategoryID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Dataset\n",
    "train_files = [f'{dataset_dir}/CarDataset-Splits-{i + 1}-Train.csv' for i in tqdm(range(num_splits), desc=\"Generating Original Train File\")]\n",
    "test_files = [f'{dataset_dir}/CarDataset-Splits-{i + 1}-Test.csv' for i in tqdm(range(num_splits), desc=\"Generating Original Test File\")]\n",
    "train_splits_with_features, test_splits_with_features = load_and_prepare_data(train_files, test_files, df_features)\n",
    "\n",
    "# Cropped Dataset\n",
    "cropped_train_files = [f'{dataset_dir}/cropped_CarDataset-Splits-{i + 1}-Train.csv' for i in tqdm(range(num_splits), desc=\"Generating Cropped Train File\")]\n",
    "cropped_test_files = [f'{dataset_dir}/cropped_CarDataset-Splits-{i + 1}-Test.csv' for i in tqdm(range(num_splits), desc=\"Generating Cropped Test File\")]\n",
    "cropped_train_splits_with_features, cropped_test_splits_with_features = load_and_prepare_data(cropped_train_files, cropped_test_files, df_cropped_features)\n",
    "\n",
    "# Cropped and Deduplicated Dataset\n",
    "cropped_dropdup_train_files = [f'{dataset_dir}/cropped_dropdup_CarDataset-Splits-{i + 1}-Train.csv' for i in tqdm(range(num_splits), desc=\"Generating Cropped Dropdup Train File\")]\n",
    "cropped_dropdup_test_files = [f'{dataset_dir}/cropped_dropdup_CarDataset-Splits-{i + 1}-Test.csv' for i in tqdm(range(num_splits), desc=\"Generating Cropped Dropdup Test File\")]\n",
    "cropped_dropdup_train_splits_with_features, cropped_dropdup_test_splits_with_features = load_and_prepare_data(cropped_dropdup_train_files, cropped_dropdup_test_files, df_cropped_dropdup_features)\n",
    "\n",
    "# Augmented Dataset\n",
    "augmented_train_files = [f'{dataset_dir}/augmented_CarDataset-Splits-{i + 1}-Train.csv' for i in tqdm(range(num_splits), desc=\"Generating Augmented Train File\")]\n",
    "augmented_test_files = [f'{dataset_dir}/augmented_CarDataset-Splits-{i + 1}-Test.csv' for i in tqdm(range(num_splits), desc=\"Generating Augmented Test File\")]\n",
    "augmented_train_splits_with_features, augmented_test_splits_with_features = load_and_prepare_data(augmented_train_files, augmented_test_files, df_augmented_features)\n",
    "\n",
    "# Full Augmented Dataset\n",
    "full_augmented_train_files = [f'{dataset_dir}/full_augmented_CarDataset-Splits-{i + 1}-Train.csv' for i in tqdm(range(num_splits), desc=\"Generating Full Augmented Train File\")]\n",
    "full_augmented_test_files = [f'{dataset_dir}/full_augmented_CarDataset-Splits-{i + 1}-Test.csv' for i in tqdm(range(num_splits), desc=\"Generating Full Augmented Test File\")]\n",
    "full_augmented_train_splits_with_features, full_augmented_test_splits_with_features = load_and_prepare_data(full_augmented_train_files, augmented_test_files, df_augmented_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Dataset\n",
    "train_df_1 = train_splits_with_features[0]\n",
    "train_df_2 = train_splits_with_features[1]\n",
    "train_df_3 = train_splits_with_features[2]\n",
    "train_df_4 = train_splits_with_features[3]\n",
    "train_df_5 = train_splits_with_features[4]\n",
    "\n",
    "test_df_1 = test_splits_with_features[0]\n",
    "test_df_2 = test_splits_with_features[1]\n",
    "test_df_3 = test_splits_with_features[2]\n",
    "test_df_4 = test_splits_with_features[3]\n",
    "test_df_5 = test_splits_with_features[4]\n",
    "\n",
    "# Cropped Dataset\n",
    "cropped_train_df_1 = cropped_train_splits_with_features[0]\n",
    "cropped_train_df_2 = cropped_train_splits_with_features[1]\n",
    "cropped_train_df_3 = cropped_train_splits_with_features[2]\n",
    "cropped_train_df_4 = cropped_train_splits_with_features[3]\n",
    "cropped_train_df_5 = cropped_train_splits_with_features[4]\n",
    "\n",
    "cropped_test_df_1 = cropped_test_splits_with_features[0]\n",
    "cropped_test_df_2 = cropped_test_splits_with_features[1]\n",
    "cropped_test_df_3 = cropped_test_splits_with_features[2]\n",
    "cropped_test_df_4 = cropped_test_splits_with_features[3]\n",
    "cropped_test_df_5 = cropped_test_splits_with_features[4]\n",
    "\n",
    "# Cropped + Drop Duplicate Dataset\n",
    "cropped_dropdup_train_df_1 = cropped_dropdup_train_splits_with_features[0]\n",
    "cropped_dropdup_train_df_2 = cropped_dropdup_train_splits_with_features[1]\n",
    "cropped_dropdup_train_df_3 = cropped_dropdup_train_splits_with_features[2]\n",
    "cropped_dropdup_train_df_4 = cropped_dropdup_train_splits_with_features[3]\n",
    "cropped_dropdup_train_df_5 = cropped_dropdup_train_splits_with_features[4]\n",
    "\n",
    "cropped_dropdup_test_df_1 = cropped_dropdup_test_splits_with_features[0]\n",
    "cropped_dropdup_test_df_2 = cropped_dropdup_test_splits_with_features[1]\n",
    "cropped_dropdup_test_df_3 = cropped_dropdup_test_splits_with_features[2]\n",
    "cropped_dropdup_test_df_4 = cropped_dropdup_test_splits_with_features[3]\n",
    "cropped_dropdup_test_df_5 = cropped_dropdup_test_splits_with_features[4]\n",
    "\n",
    "# Augmented Dataset\n",
    "augmented_train_df_1 = augmented_train_splits_with_features[0]\n",
    "augmented_train_df_2 = augmented_train_splits_with_features[1]\n",
    "augmented_train_df_3 = augmented_train_splits_with_features[2]\n",
    "augmented_train_df_4 = augmented_train_splits_with_features[3]\n",
    "augmented_train_df_5 = augmented_train_splits_with_features[4]\n",
    "\n",
    "augmented_test_df_1 = augmented_test_splits_with_features[0]\n",
    "augmented_test_df_2 = augmented_test_splits_with_features[1]\n",
    "augmented_test_df_3 = augmented_test_splits_with_features[2]\n",
    "augmented_test_df_4 = augmented_test_splits_with_features[3]\n",
    "augmented_test_df_5 = augmented_test_splits_with_features[4]\n",
    "\n",
    "# Full Augmented Dataset\n",
    "full_augmented_train_df_1 = full_augmented_train_splits_with_features[0]\n",
    "full_augmented_train_df_2 = full_augmented_train_splits_with_features[1]\n",
    "full_augmented_train_df_3 = full_augmented_train_splits_with_features[2]\n",
    "full_augmented_train_df_4 = full_augmented_train_splits_with_features[3]\n",
    "full_augmented_train_df_5 = full_augmented_train_splits_with_features[4]\n",
    "\n",
    "full_augmented_test_df_1 = full_augmented_test_splits_with_features[0]\n",
    "full_augmented_test_df_2 = full_augmented_test_splits_with_features[1]\n",
    "full_augmented_test_df_3 = full_augmented_test_splits_with_features[2]\n",
    "full_augmented_test_df_4 = full_augmented_test_splits_with_features[3]\n",
    "full_augmented_test_df_5 = full_augmented_test_splits_with_features[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_1 = np.array(train_df_1['Extracted Features'].apply(lambda x: x).tolist())\n",
    "X_train_2 = np.array(train_df_2['Extracted Features'].apply(lambda x: x).tolist())\n",
    "X_train_3 = np.array(train_df_3['Extracted Features'].apply(lambda x: x).tolist())\n",
    "X_train_4 = np.array(train_df_4['Extracted Features'].apply(lambda x: x).tolist())\n",
    "X_train_5 = np.array(train_df_5['Extracted Features'].apply(lambda x: x).tolist())\n",
    "\n",
    "X_test_1 = np.array(test_df_1['Extracted Features'].apply(lambda x: x).tolist())\n",
    "X_test_2 = np.array(test_df_2['Extracted Features'].apply(lambda x: x).tolist())\n",
    "X_test_3 = np.array(test_df_3['Extracted Features'].apply(lambda x: x).tolist())\n",
    "X_test_4 = np.array(test_df_4['Extracted Features'].apply(lambda x: x).tolist())\n",
    "X_test_5 = np.array(test_df_5['Extracted Features'].apply(lambda x: x).tolist())\n",
    "\n",
    "y_train_1 = np.array(train_df_1['CategoryID'].tolist())\n",
    "y_train_2 = np.array(train_df_2['CategoryID'].tolist())\n",
    "y_train_3 = np.array(train_df_3['CategoryID'].tolist())\n",
    "y_train_4 = np.array(train_df_4['CategoryID'].tolist())\n",
    "y_train_5 = np.array(train_df_5['CategoryID'].tolist())\n",
    "\n",
    "y_test_1 = np.array(test_df_1['CategoryID'].tolist())\n",
    "y_test_2 = np.array(test_df_2['CategoryID'].tolist())\n",
    "y_test_3 = np.array(test_df_3['CategoryID'].tolist())\n",
    "y_test_4 = np.array(test_df_4['CategoryID'].tolist())\n",
    "y_test_5 = np.array(test_df_5['CategoryID'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cropped_X_train_1 = np.array(cropped_train_df_1['Extracted Features'].apply(lambda x: x).tolist())\n",
    "cropped_X_train_2 = np.array(cropped_train_df_2['Extracted Features'].apply(lambda x: x).tolist())\n",
    "cropped_X_train_3 = np.array(cropped_train_df_3['Extracted Features'].apply(lambda x: x).tolist())\n",
    "cropped_X_train_4 = np.array(cropped_train_df_4['Extracted Features'].apply(lambda x: x).tolist())\n",
    "cropped_X_train_5 = np.array(cropped_train_df_5['Extracted Features'].apply(lambda x: x).tolist())\n",
    "\n",
    "cropped_X_test_1 = np.array(cropped_test_df_1['Extracted Features'].apply(lambda x: x).tolist())\n",
    "cropped_X_test_2 = np.array(cropped_test_df_2['Extracted Features'].apply(lambda x: x).tolist())\n",
    "cropped_X_test_3 = np.array(cropped_test_df_3['Extracted Features'].apply(lambda x: x).tolist())\n",
    "cropped_X_test_4 = np.array(cropped_test_df_4['Extracted Features'].apply(lambda x: x).tolist())\n",
    "cropped_X_test_5 = np.array(cropped_test_df_5['Extracted Features'].apply(lambda x: x).tolist())\n",
    "\n",
    "cropped_y_train_1 = np.array(cropped_train_df_1['CategoryID'].tolist())\n",
    "cropped_y_train_2 = np.array(cropped_train_df_2['CategoryID'].tolist())\n",
    "cropped_y_train_3 = np.array(cropped_train_df_3['CategoryID'].tolist())\n",
    "cropped_y_train_4 = np.array(cropped_train_df_4['CategoryID'].tolist())\n",
    "cropped_y_train_5 = np.array(cropped_train_df_5['CategoryID'].tolist())\n",
    "\n",
    "cropped_y_test_1 = np.array(cropped_test_df_1['CategoryID'].tolist())\n",
    "cropped_y_test_2 = np.array(cropped_test_df_2['CategoryID'].tolist())\n",
    "cropped_y_test_3 = np.array(cropped_test_df_3['CategoryID'].tolist())\n",
    "cropped_y_test_4 = np.array(cropped_test_df_4['CategoryID'].tolist())\n",
    "cropped_y_test_5 = np.array(cropped_test_df_5['CategoryID'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_dropdup_X_train_1 = np.array(cropped_dropdup_train_df_1['Extracted Features'].apply(lambda x: x).tolist())\n",
    "cropped_dropdup_X_train_2 = np.array(cropped_dropdup_train_df_2['Extracted Features'].apply(lambda x: x).tolist())\n",
    "cropped_dropdup_X_train_3 = np.array(cropped_dropdup_train_df_3['Extracted Features'].apply(lambda x: x).tolist())\n",
    "cropped_dropdup_X_train_4 = np.array(cropped_dropdup_train_df_4['Extracted Features'].apply(lambda x: x).tolist())\n",
    "cropped_dropdup_X_train_5 = np.array(cropped_dropdup_train_df_5['Extracted Features'].apply(lambda x: x).tolist())\n",
    "\n",
    "cropped_dropdup_X_test_1 = np.array(cropped_dropdup_test_df_1['Extracted Features'].apply(lambda x: x).tolist())\n",
    "cropped_dropdup_X_test_2 = np.array(cropped_dropdup_test_df_2['Extracted Features'].apply(lambda x: x).tolist())\n",
    "cropped_dropdup_X_test_3 = np.array(cropped_dropdup_test_df_3['Extracted Features'].apply(lambda x: x).tolist())\n",
    "cropped_dropdup_X_test_4 = np.array(cropped_dropdup_test_df_4['Extracted Features'].apply(lambda x: x).tolist())\n",
    "cropped_dropdup_X_test_5 = np.array(cropped_dropdup_test_df_5['Extracted Features'].apply(lambda x: x).tolist())\n",
    "\n",
    "cropped_dropdup_y_train_1 = np.array(cropped_dropdup_train_df_1['CategoryID'].tolist())\n",
    "cropped_dropdup_y_train_2 = np.array(cropped_dropdup_train_df_2['CategoryID'].tolist())\n",
    "cropped_dropdup_y_train_3 = np.array(cropped_dropdup_train_df_3['CategoryID'].tolist())\n",
    "cropped_dropdup_y_train_4 = np.array(cropped_dropdup_train_df_4['CategoryID'].tolist())\n",
    "cropped_dropdup_y_train_5 = np.array(cropped_dropdup_train_df_5['CategoryID'].tolist())\n",
    "\n",
    "cropped_dropdup_y_test_1 = np.array(cropped_dropdup_test_df_1['CategoryID'].tolist())\n",
    "cropped_dropdup_y_test_2 = np.array(cropped_dropdup_test_df_2['CategoryID'].tolist())\n",
    "cropped_dropdup_y_test_3 = np.array(cropped_dropdup_test_df_3['CategoryID'].tolist())\n",
    "cropped_dropdup_y_test_4 = np.array(cropped_dropdup_test_df_4['CategoryID'].tolist())\n",
    "cropped_dropdup_y_test_5 = np.array(cropped_dropdup_test_df_5['CategoryID'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_X_train_1 = np.array(augmented_train_df_1['Extracted Features'].apply(lambda x: x).tolist())\n",
    "augmented_X_train_2 = np.array(augmented_train_df_2['Extracted Features'].apply(lambda x: x).tolist())\n",
    "augmented_X_train_3 = np.array(augmented_train_df_3['Extracted Features'].apply(lambda x: x).tolist())\n",
    "augmented_X_train_4 = np.array(augmented_train_df_4['Extracted Features'].apply(lambda x: x).tolist())\n",
    "augmented_X_train_5 = np.array(augmented_train_df_5['Extracted Features'].apply(lambda x: x).tolist())\n",
    "\n",
    "augmented_X_test_1 = np.array(augmented_test_df_1['Extracted Features'].apply(lambda x: x).tolist())\n",
    "augmented_X_test_2 = np.array(augmented_test_df_2['Extracted Features'].apply(lambda x: x).tolist())\n",
    "augmented_X_test_3 = np.array(augmented_test_df_3['Extracted Features'].apply(lambda x: x).tolist())\n",
    "augmented_X_test_4 = np.array(augmented_test_df_4['Extracted Features'].apply(lambda x: x).tolist())\n",
    "augmented_X_test_5 = np.array(augmented_test_df_5['Extracted Features'].apply(lambda x: x).tolist())\n",
    "\n",
    "augmented_y_train_1 = np.array(augmented_train_df_1['CategoryID'].tolist())\n",
    "augmented_y_train_2 = np.array(augmented_train_df_2['CategoryID'].tolist())\n",
    "augmented_y_train_3 = np.array(augmented_train_df_3['CategoryID'].tolist())\n",
    "augmented_y_train_4 = np.array(augmented_train_df_4['CategoryID'].tolist())\n",
    "augmented_y_train_5 = np.array(augmented_train_df_5['CategoryID'].tolist())\n",
    "\n",
    "augmented_y_test_1 = np.array(augmented_test_df_1['CategoryID'].tolist())\n",
    "augmented_y_test_2 = np.array(augmented_test_df_2['CategoryID'].tolist())\n",
    "augmented_y_test_3 = np.array(augmented_test_df_3['CategoryID'].tolist())\n",
    "augmented_y_test_4 = np.array(augmented_test_df_4['CategoryID'].tolist())\n",
    "augmented_y_test_5 = np.array(augmented_test_df_5['CategoryID'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_augmented_X_train_1 = np.array(full_augmented_train_df_1['Extracted Features'].apply(lambda x: x).tolist())\n",
    "full_augmented_X_train_2 = np.array(full_augmented_train_df_2['Extracted Features'].apply(lambda x: x).tolist())\n",
    "full_augmented_X_train_3 = np.array(full_augmented_train_df_3['Extracted Features'].apply(lambda x: x).tolist())\n",
    "full_augmented_X_train_4 = np.array(full_augmented_train_df_4['Extracted Features'].apply(lambda x: x).tolist())\n",
    "full_augmented_X_train_5 = np.array(full_augmented_train_df_5['Extracted Features'].apply(lambda x: x).tolist())\n",
    "\n",
    "full_augmented_X_test_1 = np.array(full_augmented_test_df_1['Extracted Features'].apply(lambda x: x).tolist())\n",
    "full_augmented_X_test_2 = np.array(full_augmented_test_df_2['Extracted Features'].apply(lambda x: x).tolist())\n",
    "full_augmented_X_test_3 = np.array(full_augmented_test_df_3['Extracted Features'].apply(lambda x: x).tolist())\n",
    "full_augmented_X_test_4 = np.array(full_augmented_test_df_4['Extracted Features'].apply(lambda x: x).tolist())\n",
    "full_augmented_X_test_5 = np.array(full_augmented_test_df_5['Extracted Features'].apply(lambda x: x).tolist())\n",
    "\n",
    "full_augmented_y_train_1 = np.array(full_augmented_train_df_1['CategoryID'].tolist())\n",
    "full_augmented_y_train_2 = np.array(full_augmented_train_df_2['CategoryID'].tolist())\n",
    "full_augmented_y_train_3 = np.array(full_augmented_train_df_3['CategoryID'].tolist())\n",
    "full_augmented_y_train_4 = np.array(full_augmented_train_df_4['CategoryID'].tolist())\n",
    "full_augmented_y_train_5 = np.array(full_augmented_train_df_5['CategoryID'].tolist())\n",
    "\n",
    "full_augmented_y_test_1 = np.array(full_augmented_test_df_1['CategoryID'].tolist())\n",
    "full_augmented_y_test_2 = np.array(full_augmented_test_df_2['CategoryID'].tolist())\n",
    "full_augmented_y_test_3 = np.array(full_augmented_test_df_3['CategoryID'].tolist())\n",
    "full_augmented_y_test_4 = np.array(full_augmented_test_df_4['CategoryID'].tolist())\n",
    "full_augmented_y_test_5 = np.array(full_augmented_test_df_5['CategoryID'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tiến hành huấn luyện và đánh giá**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU\n",
    "Chỉ hoạt động với T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    (X_train_1, y_train_1, X_test_1, y_test_1),\n",
    "    (X_train_2, y_train_2, X_test_2, y_test_2),\n",
    "    (X_train_3, y_train_3, X_test_3, y_test_3),\n",
    "    (X_train_4, y_train_4, X_test_4, y_test_4),\n",
    "    (X_train_5, y_train_5, X_test_5, y_test_5)\n",
    "]\n",
    "cropped_datasets = [\n",
    "    (cropped_X_train_1, cropped_y_train_1, cropped_X_test_1, cropped_y_test_1),\n",
    "    (cropped_X_train_2, cropped_y_train_2, cropped_X_test_2, cropped_y_test_2),\n",
    "    (cropped_X_train_3, cropped_y_train_3, cropped_X_test_3, cropped_y_test_3),\n",
    "    (cropped_X_train_4, cropped_y_train_4, cropped_X_test_4, cropped_y_test_4),\n",
    "    (cropped_X_train_5, cropped_y_train_5, cropped_X_test_5, cropped_y_test_5)\n",
    "]\n",
    "\n",
    "cropped_dropdup_datasets = [\n",
    "    (cropped_dropdup_X_train_1, cropped_dropdup_y_train_1, cropped_dropdup_X_test_1, cropped_dropdup_y_test_1),\n",
    "    (cropped_dropdup_X_train_2, cropped_dropdup_y_train_2, cropped_dropdup_X_test_2, cropped_dropdup_y_test_2),\n",
    "    (cropped_dropdup_X_train_3, cropped_dropdup_y_train_3, cropped_dropdup_X_test_3, cropped_dropdup_y_test_3),\n",
    "    (cropped_dropdup_X_train_4, cropped_dropdup_y_train_4, cropped_dropdup_X_test_4, cropped_dropdup_y_test_4),\n",
    "    (cropped_dropdup_X_train_5, cropped_dropdup_y_train_5, cropped_dropdup_X_test_5, cropped_dropdup_y_test_5)\n",
    "]\n",
    "\n",
    "augmented_datasets = [\n",
    "    (augmented_X_train_1, augmented_y_train_1, augmented_X_test_1, augmented_y_test_1),\n",
    "    (augmented_X_train_2, augmented_y_train_2, augmented_X_test_2, augmented_y_test_2),\n",
    "    (augmented_X_train_3, augmented_y_train_3, augmented_X_test_3, augmented_y_test_3),\n",
    "    (augmented_X_train_4, augmented_y_train_4, augmented_X_test_4, augmented_y_test_4),\n",
    "    (augmented_X_train_5, augmented_y_train_5, augmented_X_test_5, augmented_y_test_5)\n",
    "]\n",
    "\n",
    "full_augmented_datasets = [\n",
    "    (full_augmented_X_train_1, full_augmented_y_train_1, full_augmented_X_test_1, full_augmented_y_test_1),\n",
    "    (full_augmented_X_train_2, full_augmented_y_train_2, full_augmented_X_test_2, full_augmented_y_test_2),\n",
    "    (full_augmented_X_train_3, full_augmented_y_train_3, full_augmented_X_test_3, full_augmented_y_test_3),\n",
    "    (full_augmented_X_train_4, full_augmented_y_train_4, full_augmented_X_test_4, full_augmented_y_test_4),\n",
    "    (full_augmented_X_train_5, full_augmented_y_train_5, full_augmented_X_test_5, full_augmented_y_test_5)\n",
    "]\n",
    "\n",
    "def fitting_datasets(datasets, prefix, model_name):\n",
    "    results = []\n",
    "    if model_name == 'SVM':\n",
    "        model = cuSVC(kernel='rbf', random_state=42)\n",
    "    elif model_name == 'RF':\n",
    "        model = cuRFClassifier(random_state=42)\n",
    "    else:\n",
    "        model = cuKNNClassifier(random_state=42)\n",
    "    for i, (X_train, y_train, X_test, y_test) in enumerate(tqdm(datasets, desc=f\"Training {prefix} models\")):\n",
    "        with tqdm(total=2, desc=f\"Fitting {model_name} on {prefix} Dataset {i+1}\", leave=False) as pbar:\n",
    "            model.fit(X_train, y_train)\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            pbar.update(1)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        results.append(accuracy)\n",
    "        print(f'{model_name} Accuracy for {prefix} dataset {i+1}: {accuracy:.6f}')\n",
    "    \n",
    "    average_accuracy = sum(results) / len(results)\n",
    "    print(f'Average {model_name} Accuracy for {prefix} datasets: {average_accuracy:.6f}')\n",
    "    \n",
    "    return model, results, average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_original_model, svm_original_results, svm_average_accuracy = fitting_datasets(datasets, 'original', 'SVM')\n",
    "svm_cropped_model, svm_cropped_results, svm_cropped_average_accuracy = fitting_datasets(cropped_datasets, 'cropped', 'SVM')\n",
    "svm_cropped_dropdup_model, svm_cropped_dropdup_results, svm_cropped_dropdup_average_accuracy = fitting_datasets(cropped_dropdup_datasets, 'cropped_dropdup', 'SVM')\n",
    "svm_augmented_model, svm_augmented_results, svm_augmented_average_accuracy = fitting_datasets(augmented_datasets, 'augmented', 'SVM')\n",
    "svm_full_augmented_model, svm_full_augmented_results, svm_full_augmented_average_accuracy = fitting_datasets(full_augmented_datasets, 'full_augmented', 'SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_original_model, rf_original_results, rf_average_accuracy = fitting_datasets(datasets, 'original', 'RF')\n",
    "rf_cropped_model, rf_cropped_results, rf_cropped_average_accuracy = fitting_datasets(cropped_datasets, 'cropped', 'RF')\n",
    "rf_cropped_dropdup_model, rf_cropped_dropdup_results, rf_cropped_dropdup_average_accuracy = fitting_datasets(cropped_dropdup_datasets, 'cropped_dropdup', 'RF')\n",
    "rf_augmented_model, rf_augmented_results, rf_augmented_average_accuracy = fitting_datasets(augmented_datasets, 'augmented', 'RF')\n",
    "rf_full_augmented_model, rf_full_augmented_results, rf_full_augmented_average_accuracy = fitting_datasets(full_augmented_datasets, 'full_augmented', 'RF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_original_model, knn_original_results, knn_average_accuracy = fitting_datasets(datasets, 'original', 'KNN')\n",
    "knn_cropped_model, knn_cropped_results, knn_cropped_average_accuracy = fitting_datasets(cropped_datasets, 'cropped', 'KNN')\n",
    "knn_cropped_dropdup_model, knn_cropped_dropdup_results, knn_cropped_dropdup_average_accuracy = fitting_datasets(cropped_dropdup_datasets, 'cropped_dropdup', 'KNN')\n",
    "knn_augmented_model, knn_augmented_results, knn_augmented_average_accuracy = fitting_datasets(augmented_datasets, 'augmented', 'KNN')\n",
    "knn_full_augmented_model, knn_full_augmented_results, knn_full_augmented_average_accuracy = fitting_datasets(full_augmented_datasets, 'full_augmented', 'KNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualize Kết quả**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true,\n",
    "                          y_pred,\n",
    "                          labels,\n",
    "                          title,\n",
    "                          prefix=None):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "    title = prefix + '_' + title if prefix is not None else prefix\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, X_test, y_test = datasets[-1]\n",
    "svm_y_pred = svm_original_model.predict(X_test)\n",
    "rf_y_pred = rf_original_model.predict(X_test)\n",
    "knn_y_pred = knn_original_model.predict(X_test)\n",
    "\n",
    "plot_confusion_matrix(y_test,\n",
    "                      svm_y_pred,\n",
    "                      labels=[label for label in range(len(categories))],\n",
    "                      title='Confusion Matrix for SVM (Dataset 5)',\n",
    "                      prefix='full')\n",
    "\n",
    "plot_confusion_matrix(y_test,\n",
    "                      rf_y_pred,\n",
    "                      labels=[label for label in range(len(categories))],\n",
    "                      title='Confusion Matrix for Random Forest (Dataset 5)',\n",
    "                      prefix='full')\n",
    "\n",
    "plot_confusion_matrix(y_test,\n",
    "                      knn_y_pred,\n",
    "                      labels=[label for label in range(len(categories))],\n",
    "                      title='Confusion Matrix for KNN (Dataset 5)',\n",
    "                      prefix='full')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, cropped_X_test, cropped_y_test = cropped_datasets[-1]\n",
    "svm_y_pred = svm_cropped_model.predict(cropped_X_test)\n",
    "rf_y_pred = rf_cropped_model.predict(cropped_X_test)\n",
    "knn_y_pred = knn_cropped_model.predict(cropped_X_test)\n",
    "\n",
    "plot_confusion_matrix(cropped_y_test,\n",
    "                      svm_y_pred,\n",
    "                      labels=[label for label in range(len(categories))],\n",
    "                      title='Confusion Matrix for SVM (Dataset 5)',\n",
    "                      prefix='cropped')\n",
    "\n",
    "plot_confusion_matrix(cropped_y_test,\n",
    "                      rf_y_pred,\n",
    "                      labels=[label for label in range(len(categories))],\n",
    "                      title='Confusion Matrix for Random Forest (Dataset 5)',\n",
    "                      prefix='cropped')\n",
    "\n",
    "plot_confusion_matrix(cropped_y_test,\n",
    "                      knn_y_pred,\n",
    "                      labels=[0, 1],\n",
    "                      title='Confusion Matrix for KNN (Dataset 5)',\n",
    "                      prefix='cropped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, cropped_dropdup_X_test, cropped_dropdup_y_test = cropped_dropdup_datasets[-1]\n",
    "svm_y_pred = svm_cropped_dropdup_model.predict(cropped_dropdup_X_test)\n",
    "rf_y_pred = rf_cropped_dropdup_model.predict(cropped_dropdup_X_test)\n",
    "knn_y_pred = knn_cropped_dropdup_model.predict(cropped_dropdup_X_test)\n",
    "\n",
    "plot_confusion_matrix(cropped_dropdup_y_test,\n",
    "                      svm_y_pred,\n",
    "                      labels=[label for label in range(len(categories))],\n",
    "                      title='Confusion Matrix for SVM (Dataset 5)',\n",
    "                      prefix='cropped_dropdup')\n",
    "\n",
    "plot_confusion_matrix(cropped_dropdup_y_test,\n",
    "                      rf_y_pred,\n",
    "                      labels=[label for label in range(len(categories))],\n",
    "                      title='Confusion Matrix for Random Forest (Dataset 5)',\n",
    "                      prefix='cropped_dropdup')\n",
    "\n",
    "plot_confusion_matrix(cropped_dropdup_y_test,\n",
    "                      knn_y_pred,\n",
    "                      labels=[label for label in range(len(categories))],\n",
    "                      title='Confusion Matrix for KNN (Dataset 5)',\n",
    "                      prefix='cropped_dropdup')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, augmented_X_test, augmented_y_test = augmented_datasets[-1]\n",
    "svm_y_pred = svm_augmented_model.predict(augmented_X_test)\n",
    "rf_y_pred = rf_augmented_model.predict(augmented_X_test)\n",
    "knn_y_pred = knn_augmented_model.predict(augmented_X_test)\n",
    "\n",
    "plot_confusion_matrix(augmented_y_test,\n",
    "                      svm_y_pred,\n",
    "                      labels=[label for label in range(len(categories))],\n",
    "                      title='Confusion Matrix for SVM (Dataset 5)',\n",
    "                      prefix='augmented')\n",
    "\n",
    "plot_confusion_matrix(augmented_y_test,\n",
    "                      rf_y_pred,\n",
    "                      labels=[label for label in range(len(categories))],\n",
    "                      title='Confusion Matrix for Random Forest (Dataset 5)',\n",
    "                      prefix='augmented')\n",
    "\n",
    "plot_confusion_matrix(augmented_y_test,\n",
    "                      knn_y_pred,\n",
    "                      labels=[label for label in range(len(categories))],\n",
    "                      title='Confusion Matrix for KNN (Dataset 5)',\n",
    "                      prefix='augmented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, full_augmented_X_test, full_augmented_y_test = full_augmented_datasets[-1]\n",
    "svm_y_pred = svm_full_augmented_model.predict(full_augmented_X_test)\n",
    "rf_y_pred = rf_full_augmented_model.predict(full_augmented_X_test)\n",
    "knn_y_pred = knn_full_augmented_model.predict(full_augmented_X_test)\n",
    "\n",
    "plot_confusion_matrix(full_augmented_y_test,\n",
    "                      svm_y_pred,\n",
    "                      labels=[label for label in range(len(categories))],\n",
    "                      title='Confusion Matrix for SVM (Dataset 5)',\n",
    "                      prefix='full_augmented')\n",
    "\n",
    "plot_confusion_matrix(full_augmented_y_test,\n",
    "                      rf_y_pred,\n",
    "                      labels=[label for label in range(len(categories))],\n",
    "                      title='Confusion Matrix for Random Forest (Dataset 5)',\n",
    "                      prefix='full_augmented')\n",
    "\n",
    "plot_confusion_matrix(full_augmented_y_test,\n",
    "                      knn_y_pred,\n",
    "                      labels=[label for label in range(len(categories))],\n",
    "                      title='Confusion Matrix for KNN (Dataset 5)',\n",
    "                      prefix='full_augmented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(base_dir,\n",
    "                          image_paths,\n",
    "                          y_true,\n",
    "                          model,\n",
    "                          X_test,\n",
    "                          categories,\n",
    "                          num_images=5):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    correct_indices = np.where(y_pred == y_true)[0]\n",
    "    incorrect_indices = np.where(y_pred != y_true)[0]\n",
    "\n",
    "    def display_images(indices, title):\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.suptitle(title, fontsize=16)\n",
    "        for i, idx in enumerate(random.sample(list(indices), min(num_images, len(indices)))):\n",
    "            plt.subplot(1, num_images, i + 1)\n",
    "            img_path = os.path.join(base_dir, image_paths[idx])\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"True Label: {categories[y_true[idx]]}\\nPredicted Label: {categories[y_pred[idx]]}\")\n",
    "                plt.axis(\"off\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(\"Visualizing Correctly Classified Images:\")\n",
    "    display_images(correct_indices, \"Correctly Classified Images\")\n",
    "\n",
    "    print(\"Visualizing Incorrectly Classified Images:\")\n",
    "    display_images(incorrect_indices, \"Incorrectly Classified Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(base_dir=base_dir,\n",
    "                      image_paths=test_df_1['ImageFullPath'].tolist(),\n",
    "                      y_true=y_test_1,\n",
    "                      model=svm_original_model,\n",
    "                      X_test=X_test_1,\n",
    "                      categories=categories,\n",
    "                      num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(base_dir=cropped_base_dir,\n",
    "                      image_paths=cropped_test_df_1['ImageFullPath'].tolist(),\n",
    "                      y_true=cropped_y_test_1,\n",
    "                      model=svm_cropped_model,\n",
    "                      X_test=cropped_X_test_1,\n",
    "                      categories=categories,\n",
    "                      num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(base_dir=cropped_dropdup_base_dir,\n",
    "                      image_paths=cropped_dropdup_test_df_1['ImageFullPath'].tolist(),\n",
    "                      y_true=cropped_dropdup_y_test_1,\n",
    "                      model=svm_cropped_dropdup_model,\n",
    "                      X_test=cropped_dropdup_X_test_1,\n",
    "                      categories=categories,\n",
    "                      num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(base_dir=augmented_base_dir,\n",
    "                      image_paths=augmented_test_df_1['ImageFullPath'].tolist(),\n",
    "                      y_true=augmented_y_test_1,\n",
    "                      model=svm_augmented_model,\n",
    "                      X_test=augmented_X_test_1,\n",
    "                      categories=categories,\n",
    "                      num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(base_dir=full_augmented_base_dir,\n",
    "                      image_paths=full_augmented_test_df_1['ImageFullPath'].tolist(),\n",
    "                      y_true=full_augmented_y_test_1,\n",
    "                      model=svm_full_augmented_model,\n",
    "                      X_test=full_augmented_X_test_1,\n",
    "                      categories=categories,\n",
    "                      num_images=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
