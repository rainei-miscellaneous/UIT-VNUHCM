{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwbwjOLpyLEi"
   },
   "source": [
    "\n",
    "# TIỀN XỬ LÝ DỮ LIỆU (PHẦN 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOyTiugYx9BC"
   },
   "source": [
    "Trong lĩnh vực machine learning, việc tiền xử lý dữ liệu đóng vai trò quan trọng để xây dựng những mô hình chính xác và mạnh mẽ. Dữ liệu thô thường gặp phải tình trạng lộn xộn, thiếu sót và không nhất quán. Tiền xử lý dữ liệu bao gồm một loạt các kỹ thuật và nhiệm vụ nhằm biến đổi và tinh chỉnh dữ liệu thô thành một định dạng thích hợp, chuẩn bị cho việc phân tích và huấn luyện mô hình.\n",
    "\n",
    "Bằng cách giải quyết các vấn đề như giá trị thiếu, các ngoại lệ và tỷ lệ biến đổi khác nhau, việc tiền xử lý đảm bảo chất lượng dữ liệu được cải thiện, dẫn đến hiệu suất mô hình cải thiện.\n",
    "\n",
    "Trong cuốn sổ tay Jupyter này, chúng ta sẽ đào sâu vào các bước cơ bản của tiền xử lý dữ liệu. Chúng ta sẽ làm việc thông qua một bài tập thực tế bằng Python, nơi chúng ta sẽ tập trung vào 2 nhiệm vụ cơ bản:\n",
    "\n",
    "* Xác định và xử lý giá trị bị thiếu\n",
    "* Chuẩn hóa dữ liệu\n",
    "\n",
    "Khi kết thúc bài tập này, bạn sẽ đã có được kinh nghiệm thực tế trong việc đánh giá, làm sạch và biến đổi dữ liệu thô thành một định dạng thích hợp cho việc học máy. Những kỹ năng này là nền tảng trong hành trình khoa học dữ liệu, vì chúng đặt ra sân khấu cho các kỹ thuật nâng cao hơn và việc xây dựng mô hình. Vậy thì, hãy bắt đầu và học nghệ thuật tiền xử lý dữ liệu để mở khóa tiềm năng thực sự của những nỗ lực trong lĩnh vực học máy của bạn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsjvfCQwx9BF"
   },
   "source": [
    "### 1. Tải dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6U-6vxuHx9BG"
   },
   "source": [
    "Dữ liệu được sử dụng sẽ là dữ liệu về giá nhà, cụ thể như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "IPTB2Qa5x9BG",
    "outputId": "e0b68d03-20a9-4023-9099-3d66bd03ad8d"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/housing.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/housing.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m df_test \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      6\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\ASUS Vivobook\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS Vivobook\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\ASUS Vivobook\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS Vivobook\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\ASUS Vivobook\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/housing.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"./data/housing.csv\"\n",
    "df = pd.read_csv(data_path, index_col=0)\n",
    "df_test = df.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqDzzUZwx9BH"
   },
   "source": [
    "### 2. Xác định số phần tử bị thiếu ở mỗi cột"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPWJvxZIx9BH",
    "outputId": "97b17c53-df7c-4996-d0f3-2d80e54a567b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude              6826\n",
       "latitude               5020\n",
       "housing_median_age     7711\n",
       "total_rooms           13089\n",
       "total_bedrooms          432\n",
       "population            13206\n",
       "households            15149\n",
       "median_income          2145\n",
       "median_house_value     5227\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Liệt kê số phần tử bị thiếu ở mỗi cột\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqKArO73x9BI"
   },
   "source": [
    "### 3. Loại bỏ những cột có nhiều giá trị bị thiếu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pmc9XRstx9BI"
   },
   "source": [
    "**Bài tập**: Hãy viết hàm nhận vào `dataframe` và `threshold` (ngưỡng phần trăm).\n",
    "Trả về `dataframe` mới sau khi đã loại bỏ hết tất cả các cột mà tỉ lệ phần\n",
    "trăm giá trị bị thiếu vượt qua `threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8_TD6b8x9BI"
   },
   "outputs": [],
   "source": [
    "def drop_sparse_columns(df: pd.DataFrame, threshold: float) -> pd.DataFrame:\n",
    "\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    percentage = df.isna().mean()\n",
    "    col = percentage[percentage > threshold].index\n",
    "    return df.drop(columns = col)\n",
    "    ### END SOLUTION\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCLhsP8kx9BI"
   },
   "source": [
    "Ta tiến hành loại bỏ những cột có nhiều giá trị bị thiếu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZnCp4H6x9BJ"
   },
   "outputs": [],
   "source": [
    "# Nếu cột có phần trăm giá trị bị thiếu > 60% thì sẽ bị loại bỏ\n",
    "threshold = 0.6\n",
    "df = drop_sparse_columns(df, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLWlfDJLx9BJ"
   },
   "outputs": [],
   "source": [
    "# Kiểm tra với public tests\n",
    "assert df.shape[1] == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvD4ssSNx9BJ",
    "outputId": "4c6602a5-c6e7-4062-f54a-edac34d65692"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude             6826\n",
       "latitude              5020\n",
       "housing_median_age    7711\n",
       "total_bedrooms         432\n",
       "median_income         2145\n",
       "median_house_value    5227\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sau khi đã loại bỏ những cột không cần thiết do chứa quá nhiều giá trị bị thiếu\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfDyzmQ9x9BJ"
   },
   "source": [
    "### 4. Lắp đầy những giá trị thiếu ở những cột còn lại"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFQ6zJtbx9BJ"
   },
   "source": [
    "**Bài tập**: Hãy viết các hàm thực hiện điền giá trị bị thiếu vào `dataframe` ứng với\n",
    "với các chiến lược sau: ***min imputation***, ***max imputation***, ***mean imputation***, ***zero imputation***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8cXz3K6x9BJ"
   },
   "outputs": [],
   "source": [
    "def scale_test(df: pd.DataFrame, scaler) -> pd.DataFrame:\n",
    "    scaled_data = df.apply(lambda col:\n",
    "                           scaler.fit_transform(col.values.reshape(-1, 1)).flatten())\n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "    return scaled_df\n",
    "    pass\n",
    "\n",
    "def fill_with_min(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    return df.fillna(df.min())\n",
    "    ### END SOLUTION\n",
    "    pass\n",
    "\n",
    "def fill_with_max(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    return df.fillna(df.max())\n",
    "    ### END SOLUTION\n",
    "    pass\n",
    "\n",
    "def fill_with_mean(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    return df.fillna(df.mean())\n",
    "    ### END SOLUTION\n",
    "    pass\n",
    "\n",
    "def fill_with_zero(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    return df.fillna(0)\n",
    "    ### END SOLUTION\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48xPx7QOx9BK"
   },
   "source": [
    "Ta gọi hàm và tạo những `dataframe` mới ứng với từng kiểu điền rỗng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzXSFk24x9BK"
   },
   "outputs": [],
   "source": [
    "min_filled_df = fill_with_min(df)\n",
    "max_filled_df = fill_with_max(df)\n",
    "mean_filled_df = fill_with_mean(df)\n",
    "zero_filled_df = fill_with_zero(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u15m8awox9BK"
   },
   "outputs": [],
   "source": [
    "# Kiểm tra với public tests\n",
    "assert not min_filled_df.isna().any().any()\n",
    "assert not max_filled_df.isna().any().any()\n",
    "assert not mean_filled_df.isna().any().any()\n",
    "assert not zero_filled_df.isna().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHlRCFAKx9BK"
   },
   "source": [
    "## 4. Chuẩn hóa dữ liệu\n",
    "Các đặc trưng thường đi kèm với các tỷ lệ biến đổi khác nhau, điều này có thể dẫn đến mô hình thiên vị. Chúng ta sẽ khám phá các kỹ thuật chuẩn hóa phổ biến\n",
    "\n",
    "- Min-Max Scaling: Nó biến đổi các giá trị trong tập dữ liệu về các giá trị trong khoảng từ 0 đến 1.\n",
    "$$ x_{scaled} = {x-x_{min} \\over x_{max} - x_{min}} $$\n",
    "\n",
    "\n",
    ">>>| x | $x_{scaled}$ |\n",
    "|:--------:|:--------:|\n",
    "| 10       | 0.0      |\n",
    "| -20       | 0.5      |\n",
    "| 35       | 0.25      |\n",
    "| 48       | 1.0      |\n",
    "| 53       | 0.75      |\n",
    "\n",
    "- Standard Scaling (Z-score normalization): Nó tính toán giá trị trung bình và độ lệch chuẩn của tập dữ liệu và chuẩn hóa nó bằng cách trừ giá trị trung bình và chia cho độ lệch chuẩn.\n",
    "\n",
    "$$ x_{scaled} = {x- mean_x \\over std_x} $$\n",
    "\n",
    ">>>| x | $x_{scaled}$ |\n",
    "|:--------:|:--------:|\n",
    "| 10       | -0.56     |\n",
    "| -20       | -1.67      |\n",
    "| 35       | 0.36      |\n",
    "| 48       | 0.84     |\n",
    "| 53       | 1.03      |\n",
    "\n",
    " >>>$mean_x=$25.2, $std_x \\approx$27.0658\n",
    "\n",
    " >>>$mean_{x_{scaled}} \\approx$0, $std_{x_{scaled}} \\approx$1\n",
    "\n",
    "- Robust Scaling: RobustScaler là một kỹ thuật sử dụng trung vị và quartiles để giải quyết các bias từ các giá trị ngoại lệ.\n",
    "\n",
    "$$ x_{scaled} = {x-x_{median} \\over x_{75} - x_{25}} $$\n",
    "\n",
    ">>>| x | $x_{scaled}$ |\n",
    "|:--------:|:--------:|\n",
    "| 10       | -0.66     |\n",
    "| -20       | -1.45      |\n",
    "| 35       | 0.0      |\n",
    "| 48       | 0.34     |\n",
    "| 53       | 0.47      |\n",
    "\n",
    "![anh](https://i.imgur.com/MARX2bg.png)\n",
    "\n",
    " Những kỹ thuật này sẽ giúp đưa các đặc trưng về một tỷ lệ chung, ngăn chặn bất kỳ đặc trưng nào chiếm ưu thế trong quá trình học."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aByZhZ4Dx9BK"
   },
   "source": [
    "**Bài tập**: Hãy viết hàm nhận vào `dataframe` và một `object` thuộc một trong ba\n",
    "scaler đã được import bên dưới và trả vể dataframe đã được chuẩn hóa sử dụng scaler đó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmQLxzz-x9BK"
   },
   "outputs": [],
   "source": [
    "# Sử dụng các class scaler có từ thư viện sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoxgX0Buw68x",
    "outputId": "f6c11483-6c0e-45e6-bc7d-d44722cfcb41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Column1\n",
      "0 -0.657895\n",
      "1 -1.447368\n",
      "2  0.000000\n",
      "3  0.342105\n",
      "4  0.473684\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Tạo DataFrame với một cột dữ liệu\n",
    "data = {'Column1': [10, -20, 35, 48, 53]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Khởi tạo StandardScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Áp dụng StandardScaler vào cột dữ liệu\n",
    "scaled_data = df.apply(lambda col: scaler.fit_transform(col.values.reshape(-1, 1)).flatten())\n",
    "\n",
    "# In kết quả\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hCRLIGlx9BK"
   },
   "outputs": [],
   "source": [
    "def scale(df: pd.DataFrame, scaler) -> pd.DataFrame:\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    scaler_arr = scaler.fit_transform(df)\n",
    "\n",
    "    scaled_df = pd.DataFrame(scaler_arr, index = df.index, columns = df.columns)\n",
    "\n",
    "    return scaled_df\n",
    "\n",
    "    ### END SOLUTION\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqLvNEYlx9BK"
   },
   "source": [
    "Tiến hành tạo các `dataframe` ứng với từng kiểu chuẩn hóa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IshiE5CBx9BL"
   },
   "outputs": [],
   "source": [
    "minmax_scaled_df = scale(mean_filled_df, MinMaxScaler())\n",
    "standard_scaled_df = scale(mean_filled_df, StandardScaler())\n",
    "robust_scaled_df = scale(mean_filled_df, RobustScaler())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
